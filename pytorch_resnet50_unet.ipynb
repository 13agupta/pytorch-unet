{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image, Right: Target mask\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 2000, 'val': 200}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 3, 192, 192]) torch.Size([25, 6, 192, 192])\n",
      "-2.117904 2.64 -1.880778 0.6873421\n",
      "0.0 1.0 0.004816081 0.069230616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f15ea83d2e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (3): Bottleneck(\n",
       "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (3): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (4): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (5): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " AvgPool2d(kernel_size=7, stride=1, padding=0),\n",
       " Linear(in_features=2048, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "base_model = models.resnet50(pretrained=False)\n",
    "    \n",
    "list(base_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "       AvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "summary(base_model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 256, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(256, 256, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 512, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 1024, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(1024, 512, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 2048, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(2048, 1024, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(512 + 1024, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(512 + 512, 512, 3, 1)\n",
    "        self.conv_up1 = convrelu(256 + 512, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(input)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]           4,096\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "             ReLU-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-16          [-1, 256, 56, 56]             512\n",
      "           Conv2d-17          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-18          [-1, 256, 56, 56]             512\n",
      "             ReLU-19          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-20          [-1, 256, 56, 56]               0\n",
      "           Conv2d-21           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "           Conv2d-24           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-25           [-1, 64, 56, 56]             128\n",
      "             ReLU-26           [-1, 64, 56, 56]               0\n",
      "           Conv2d-27          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-28          [-1, 256, 56, 56]             512\n",
      "             ReLU-29          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-30          [-1, 256, 56, 56]               0\n",
      "           Conv2d-31           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
      "             ReLU-33           [-1, 64, 56, 56]               0\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "             ReLU-36           [-1, 64, 56, 56]               0\n",
      "           Conv2d-37          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-38          [-1, 256, 56, 56]             512\n",
      "             ReLU-39          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-40          [-1, 256, 56, 56]               0\n",
      "           Conv2d-41          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-42          [-1, 128, 56, 56]             256\n",
      "             ReLU-43          [-1, 128, 56, 56]               0\n",
      "           Conv2d-44          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 28, 28]             256\n",
      "             ReLU-46          [-1, 128, 28, 28]               0\n",
      "           Conv2d-47          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-48          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-49          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-50          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-51          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-52          [-1, 512, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "             ReLU-58          [-1, 128, 28, 28]               0\n",
      "           Conv2d-59          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-61          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-62          [-1, 512, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-71          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-72          [-1, 512, 28, 28]               0\n",
      "           Conv2d-73          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-74          [-1, 128, 28, 28]             256\n",
      "             ReLU-75          [-1, 128, 28, 28]               0\n",
      "           Conv2d-76          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-77          [-1, 128, 28, 28]             256\n",
      "             ReLU-78          [-1, 128, 28, 28]               0\n",
      "           Conv2d-79          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-80          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-81          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-82          [-1, 512, 28, 28]               0\n",
      "           Conv2d-83          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-84          [-1, 256, 28, 28]             512\n",
      "             ReLU-85          [-1, 256, 28, 28]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-90         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-91         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-92         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-93         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-94         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-95          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-96          [-1, 256, 14, 14]             512\n",
      "             ReLU-97          [-1, 256, 14, 14]               0\n",
      "           Conv2d-98          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 14, 14]             512\n",
      "            ReLU-100          [-1, 256, 14, 14]               0\n",
      "          Conv2d-101         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-102         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-103         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-104         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "            ReLU-107          [-1, 256, 14, 14]               0\n",
      "          Conv2d-108          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-109          [-1, 256, 14, 14]             512\n",
      "            ReLU-110          [-1, 256, 14, 14]               0\n",
      "          Conv2d-111         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-112         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-113         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-114         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-115          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-116          [-1, 256, 14, 14]             512\n",
      "            ReLU-117          [-1, 256, 14, 14]               0\n",
      "          Conv2d-118          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-119          [-1, 256, 14, 14]             512\n",
      "            ReLU-120          [-1, 256, 14, 14]               0\n",
      "          Conv2d-121         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-122         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-123         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-124         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-125          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-126          [-1, 256, 14, 14]             512\n",
      "            ReLU-127          [-1, 256, 14, 14]               0\n",
      "          Conv2d-128          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-129          [-1, 256, 14, 14]             512\n",
      "            ReLU-130          [-1, 256, 14, 14]               0\n",
      "          Conv2d-131         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-132         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-133         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-134         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-135          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-136          [-1, 256, 14, 14]             512\n",
      "            ReLU-137          [-1, 256, 14, 14]               0\n",
      "          Conv2d-138          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-139          [-1, 256, 14, 14]             512\n",
      "            ReLU-140          [-1, 256, 14, 14]               0\n",
      "          Conv2d-141         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-142         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-143         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-144         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-145          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-146          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-147          [-1, 512, 14, 14]               0\n",
      "          Conv2d-148            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-149            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-150            [-1, 512, 7, 7]               0\n",
      "          Conv2d-151           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-152           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-153           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-154           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-155           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-156           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-157            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-158            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-159            [-1, 512, 7, 7]               0\n",
      "          Conv2d-160            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-161            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-162            [-1, 512, 7, 7]               0\n",
      "          Conv2d-163           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-165           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-166           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-167            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-168            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-169            [-1, 512, 7, 7]               0\n",
      "          Conv2d-170            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-171            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-172            [-1, 512, 7, 7]               0\n",
      "          Conv2d-173           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-174           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-175           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-176           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-177           [-1, 1024, 7, 7]       2,098,176\n",
      "            ReLU-178           [-1, 1024, 7, 7]               0\n",
      "        Upsample-179         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-180          [-1, 512, 14, 14]         524,800\n",
      "            ReLU-181          [-1, 512, 14, 14]               0\n",
      "          Conv2d-182          [-1, 512, 14, 14]       7,078,400\n",
      "            ReLU-183          [-1, 512, 14, 14]               0\n",
      "        Upsample-184          [-1, 512, 28, 28]               0\n",
      "          Conv2d-185          [-1, 512, 28, 28]         262,656\n",
      "            ReLU-186          [-1, 512, 28, 28]               0\n",
      "          Conv2d-187          [-1, 512, 28, 28]       4,719,104\n",
      "            ReLU-188          [-1, 512, 28, 28]               0\n",
      "        Upsample-189          [-1, 512, 56, 56]               0\n",
      "          Conv2d-190          [-1, 256, 56, 56]          65,792\n",
      "            ReLU-191          [-1, 256, 56, 56]               0\n",
      "          Conv2d-192          [-1, 256, 56, 56]       1,769,728\n",
      "            ReLU-193          [-1, 256, 56, 56]               0\n",
      "        Upsample-194        [-1, 256, 112, 112]               0\n",
      "          Conv2d-195         [-1, 64, 112, 112]           4,160\n",
      "            ReLU-196         [-1, 64, 112, 112]               0\n",
      "          Conv2d-197        [-1, 128, 112, 112]         368,768\n",
      "            ReLU-198        [-1, 128, 112, 112]               0\n",
      "        Upsample-199        [-1, 128, 224, 224]               0\n",
      "          Conv2d-200         [-1, 64, 224, 224]         110,656\n",
      "            ReLU-201         [-1, 64, 224, 224]               0\n",
      "          Conv2d-202          [-1, 6, 224, 224]             390\n",
      "================================================================\n",
      "Total params: 40,549,382\n",
      "Trainable params: 40,549,382\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(6)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.070256, dice: 0.856320, loss: 0.463288\n",
      "val: bce: 0.014897, dice: 0.515814, loss: 0.265356\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 1/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.011369, dice: 0.309445, loss: 0.160407\n",
      "val: bce: 0.003790, dice: 0.113682, loss: 0.058736\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 2/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003480, dice: 0.089928, loss: 0.046704\n",
      "val: bce: 0.002525, dice: 0.067604, loss: 0.035064\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 3/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002640, dice: 0.059826, loss: 0.031233\n",
      "val: bce: 0.002230, dice: 0.059516, loss: 0.030873\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 4/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002151, dice: 0.048005, loss: 0.025078\n",
      "val: bce: 0.001772, dice: 0.048981, loss: 0.025377\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 5/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001779, dice: 0.039846, loss: 0.020812\n",
      "val: bce: 0.001648, dice: 0.044195, loss: 0.022922\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 6/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001570, dice: 0.035413, loss: 0.018492\n",
      "val: bce: 0.001770, dice: 0.044250, loss: 0.023010\n",
      "0m 50s\n",
      "Epoch 7/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001363, dice: 0.031970, loss: 0.016667\n",
      "val: bce: 0.001791, dice: 0.042280, loss: 0.022035\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 8/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001216, dice: 0.029510, loss: 0.015363\n",
      "val: bce: 0.001636, dice: 0.040241, loss: 0.020939\n",
      "saving best model\n",
      "0m 50s\n",
      "Epoch 9/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001102, dice: 0.026798, loss: 0.013950\n",
      "val: bce: 0.001550, dice: 0.038254, loss: 0.019902\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 10/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001022, dice: 0.024261, loss: 0.012641\n",
      "val: bce: 0.001892, dice: 0.038144, loss: 0.020018\n",
      "0m 51s\n",
      "Epoch 11/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000966, dice: 0.023030, loss: 0.011998\n",
      "val: bce: 0.001807, dice: 0.040560, loss: 0.021183\n",
      "0m 51s\n",
      "Epoch 12/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000959, dice: 0.022332, loss: 0.011646\n",
      "val: bce: 0.001545, dice: 0.037748, loss: 0.019646\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 13/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000954, dice: 0.021886, loss: 0.011420\n",
      "val: bce: 0.001583, dice: 0.035522, loss: 0.018553\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 14/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000903, dice: 0.020732, loss: 0.010818\n",
      "val: bce: 0.001605, dice: 0.034511, loss: 0.018058\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 15/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000875, dice: 0.020229, loss: 0.010552\n",
      "val: bce: 0.001735, dice: 0.036320, loss: 0.019028\n",
      "0m 51s\n",
      "Epoch 16/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000846, dice: 0.019502, loss: 0.010174\n",
      "val: bce: 0.001603, dice: 0.035112, loss: 0.018357\n",
      "0m 51s\n",
      "Epoch 17/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000828, dice: 0.018908, loss: 0.009868\n",
      "val: bce: 0.001738, dice: 0.035582, loss: 0.018660\n",
      "0m 50s\n",
      "Epoch 18/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000786, dice: 0.018146, loss: 0.009466\n",
      "val: bce: 0.001639, dice: 0.035409, loss: 0.018524\n",
      "0m 50s\n",
      "Epoch 19/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000778, dice: 0.017965, loss: 0.009372\n",
      "val: bce: 0.001559, dice: 0.034482, loss: 0.018021\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 20/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000776, dice: 0.017846, loss: 0.009311\n",
      "val: bce: 0.001529, dice: 0.033766, loss: 0.017648\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 21/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000786, dice: 0.017658, loss: 0.009222\n",
      "val: bce: 0.001434, dice: 0.033253, loss: 0.017343\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 22/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000761, dice: 0.017041, loss: 0.008901\n",
      "val: bce: 0.001787, dice: 0.034738, loss: 0.018262\n",
      "0m 50s\n",
      "Epoch 23/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000743, dice: 0.016727, loss: 0.008735\n",
      "val: bce: 0.001595, dice: 0.032887, loss: 0.017241\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 24/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000762, dice: 0.017094, loss: 0.008928\n",
      "val: bce: 0.001476, dice: 0.031623, loss: 0.016550\n",
      "saving best model\n",
      "0m 50s\n",
      "Epoch 25/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000738, dice: 0.016564, loss: 0.008651\n",
      "val: bce: 0.001653, dice: 0.031632, loss: 0.016643\n",
      "0m 51s\n",
      "Epoch 26/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000787, dice: 0.017148, loss: 0.008967\n",
      "val: bce: 0.001526, dice: 0.032351, loss: 0.016938\n",
      "0m 51s\n",
      "Epoch 27/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000730, dice: 0.016158, loss: 0.008444\n",
      "val: bce: 0.001611, dice: 0.032429, loss: 0.017020\n",
      "0m 51s\n",
      "Epoch 28/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000690, dice: 0.015567, loss: 0.008128\n",
      "val: bce: 0.001666, dice: 0.033520, loss: 0.017593\n",
      "0m 51s\n",
      "Epoch 29/59\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.000671, dice: 0.015082, loss: 0.007877\n",
      "val: bce: 0.001575, dice: 0.033148, loss: 0.017362\n",
      "0m 51s\n",
      "Epoch 30/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000641, dice: 0.014402, loss: 0.007521\n",
      "val: bce: 0.001460, dice: 0.031846, loss: 0.016653\n",
      "0m 50s\n",
      "Epoch 31/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000600, dice: 0.013728, loss: 0.007164\n",
      "val: bce: 0.001452, dice: 0.031908, loss: 0.016680\n",
      "0m 50s\n",
      "Epoch 32/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000592, dice: 0.013455, loss: 0.007023\n",
      "val: bce: 0.001445, dice: 0.031674, loss: 0.016559\n",
      "0m 51s\n",
      "Epoch 33/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000574, dice: 0.013168, loss: 0.006871\n",
      "val: bce: 0.001431, dice: 0.031539, loss: 0.016485\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 34/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000569, dice: 0.012981, loss: 0.006775\n",
      "val: bce: 0.001428, dice: 0.031582, loss: 0.016505\n",
      "0m 51s\n",
      "Epoch 35/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000564, dice: 0.012798, loss: 0.006681\n",
      "val: bce: 0.001429, dice: 0.031574, loss: 0.016502\n",
      "0m 51s\n",
      "Epoch 36/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000561, dice: 0.012605, loss: 0.006583\n",
      "val: bce: 0.001436, dice: 0.031709, loss: 0.016572\n",
      "0m 51s\n",
      "Epoch 37/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000556, dice: 0.012459, loss: 0.006508\n",
      "val: bce: 0.001432, dice: 0.031532, loss: 0.016482\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 38/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000554, dice: 0.012263, loss: 0.006408\n",
      "val: bce: 0.001444, dice: 0.031641, loss: 0.016543\n",
      "0m 51s\n",
      "Epoch 39/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000555, dice: 0.012101, loss: 0.006328\n",
      "val: bce: 0.001453, dice: 0.031530, loss: 0.016492\n",
      "0m 51s\n",
      "Epoch 40/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000557, dice: 0.011972, loss: 0.006265\n",
      "val: bce: 0.001473, dice: 0.031550, loss: 0.016511\n",
      "0m 51s\n",
      "Epoch 41/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000556, dice: 0.011858, loss: 0.006207\n",
      "val: bce: 0.001465, dice: 0.031489, loss: 0.016477\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 42/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000554, dice: 0.011740, loss: 0.006147\n",
      "val: bce: 0.001456, dice: 0.031327, loss: 0.016391\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 43/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000549, dice: 0.011589, loss: 0.006069\n",
      "val: bce: 0.001469, dice: 0.031460, loss: 0.016464\n",
      "0m 50s\n",
      "Epoch 44/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000551, dice: 0.011502, loss: 0.006026\n",
      "val: bce: 0.001474, dice: 0.031306, loss: 0.016390\n",
      "saving best model\n",
      "0m 50s\n",
      "Epoch 45/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000550, dice: 0.011395, loss: 0.005972\n",
      "val: bce: 0.001480, dice: 0.031193, loss: 0.016336\n",
      "saving best model\n",
      "0m 50s\n",
      "Epoch 46/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000548, dice: 0.011287, loss: 0.005917\n",
      "val: bce: 0.001485, dice: 0.031277, loss: 0.016381\n",
      "0m 50s\n",
      "Epoch 47/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000546, dice: 0.011192, loss: 0.005869\n",
      "val: bce: 0.001495, dice: 0.031401, loss: 0.016448\n",
      "0m 51s\n",
      "Epoch 48/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000546, dice: 0.011096, loss: 0.005821\n",
      "val: bce: 0.001474, dice: 0.031231, loss: 0.016352\n",
      "0m 50s\n",
      "Epoch 49/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000542, dice: 0.011033, loss: 0.005787\n",
      "val: bce: 0.001512, dice: 0.031240, loss: 0.016376\n",
      "0m 50s\n",
      "Epoch 50/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000540, dice: 0.010925, loss: 0.005732\n",
      "val: bce: 0.001527, dice: 0.031299, loss: 0.016413\n",
      "0m 51s\n",
      "Epoch 51/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000538, dice: 0.010794, loss: 0.005666\n",
      "val: bce: 0.001509, dice: 0.031244, loss: 0.016377\n",
      "0m 50s\n",
      "Epoch 52/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000535, dice: 0.010693, loss: 0.005614\n",
      "val: bce: 0.001516, dice: 0.031133, loss: 0.016324\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 53/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000531, dice: 0.010627, loss: 0.005579\n",
      "val: bce: 0.001517, dice: 0.031027, loss: 0.016272\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 54/59\n",
      "----------\n",
      "LR 1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: bce: 0.000529, dice: 0.010534, loss: 0.005532\n",
      "val: bce: 0.001527, dice: 0.031000, loss: 0.016264\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 55/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000524, dice: 0.010475, loss: 0.005500\n",
      "val: bce: 0.001527, dice: 0.030887, loss: 0.016207\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 56/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000523, dice: 0.010416, loss: 0.005469\n",
      "val: bce: 0.001524, dice: 0.030817, loss: 0.016171\n",
      "saving best model\n",
      "0m 51s\n",
      "Epoch 57/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000523, dice: 0.010289, loss: 0.005406\n",
      "val: bce: 0.001558, dice: 0.030965, loss: 0.016261\n",
      "0m 51s\n",
      "Epoch 58/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000518, dice: 0.010209, loss: 0.005364\n",
      "val: bce: 0.001548, dice: 0.031034, loss: 0.016291\n",
      "0m 51s\n",
      "Epoch 59/59\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.000518, dice: 0.010168, loss: 0.005343\n",
      "val: bce: 0.001566, dice: 0.030785, loss: 0.016176\n",
      "0m 50s\n",
      "Best val loss: 0.016171\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 6\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "#for l in model.base_layers:\n",
    "#    for param in l.parameters():\n",
    "#        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)        \n",
    "        \n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 192, 192)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "pred = F.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
