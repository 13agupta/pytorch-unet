{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "# other_input_images = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "# other_target_masks = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "# print(np.array(input_images_rgb).shape)\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image, Right: Target mask\n",
    "helper.plot_side_by_side([input_images, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs = '/A/'\n",
    "# files = os.listdir(dirs)\n",
    "# data = [imread(files[i]) for i in range(len(files))]\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms, datasets, models\n",
    "\n",
    "# class SimDataset(Dataset):\n",
    "#     def __init__(self, count, transform=None):\n",
    "#         self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)        \n",
    "#         self.transform = transform\n",
    "#         print(self.input_images.shape)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.input_images)\n",
    "    \n",
    "#     def __getitem__(self, idx):        \n",
    "#         image = self.input_images[idx]\n",
    "#         mask = self.target_masks[idx]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         return [image, mask]\n",
    "\n",
    "# # use same transform for train/val for this example\n",
    "# trans = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "# ])\n",
    "\n",
    "# train_set = SimDataset(2000, transform=trans)\n",
    "# val_set = SimDataset(200, transform=trans)\n",
    "\n",
    "# image_datasets = {\n",
    "#     'train': train_set, 'val': val_set\n",
    "# }\n",
    "\n",
    "# batch_size = 25\n",
    "\n",
    "# dataloaders = {\n",
    "#     'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "#     'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# }\n",
    "\n",
    "# dataset_sizes = {\n",
    "#     x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "# }\n",
    "\n",
    "# dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 256, 256, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 1500, 'val': 500}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function that reshapes the input image to have 1 as the number of channels\n",
    "def makeInput(image):\n",
    "    # print(image.shape)\n",
    "    return image.reshape((image.shape[0], image.shape[1], 1))\n",
    "\n",
    "# ROIList = [2, 3, 41, 42]\n",
    "def makeSeg(image):\n",
    "    \n",
    "    image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "    foreground = np.zeros(image.shape)\n",
    "    first = np.zeros(image.shape)\n",
    "    first[image == 2] == 1\n",
    "    foreground[image == 2] == 1\n",
    "    second = np.zeros(image.shape)\n",
    "    second[image == 3] == 1\n",
    "    foreground[image == 3] == 1\n",
    "    third = np.zeros(image.shape)\n",
    "    third[image == 41] == 1\n",
    "    foreground[image == 41] == 1\n",
    "    fourth = np.zeros(image.shape)\n",
    "    fourth[image == 42] == 1\n",
    "    foreground[image == 42] == 1\n",
    "    fifth = np.zeros(image.shape)\n",
    "    fifth[foreground == 0] == 1\n",
    "    \n",
    "    return np.concatenate((first, second, third, fourth, fifth), axis=2)\n",
    "    \n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        if train:\n",
    "            dirs = './original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            # print(files[0])\n",
    "            self.input_images = np.array([makeInput(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            print(self.input_images.shape)\n",
    "            dirs = './segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        else:\n",
    "            dirs = './validation_original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.input_images = np.array([makeInput(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            dirs = './validation_segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            transformation = self.transform\n",
    "            image = transformation(image)\n",
    "            mask = transformation(mask)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Resize(192), # Added this line\n",
    "    # transforms.RandomCrop(180), # Added this line\n",
    "    # transforms.RandomRotation(10), # Added this line\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(train=True, transform=trans)\n",
    "val_set = SimDataset(train=False, transform=trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 1, 256, 256]) torch.Size([25, 5, 256, 256])\n",
      "-2.117904 2.2489083 -1.5275267 0.86957\n",
      "-2.1179039301310043 0.0 -1.1916125320579487 0.9783626455077156\n"
     ]
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "# plt.imshow(reverse_transform(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AvgPool2d(kernel_size=7, stride=1, padding=0),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "    \n",
    "list(base_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "        AvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 107.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "summary(base_model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layersi = convrelu(1, 3,1,0)\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        layersi = self.layersi(input)\n",
    "        x_original = self.conv_original_size0(layersi)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(layersi)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajay/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]               6\n",
      "              ReLU-2          [-1, 3, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]           1,792\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "            Conv2d-5         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "            Conv2d-7         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-8         [-1, 64, 128, 128]             128\n",
      "              ReLU-9         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-10           [-1, 64, 64, 64]               0\n",
      "           Conv2d-11           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 64, 64]             128\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "           Conv2d-14           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 64, 64]             128\n",
      "             ReLU-16           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-17           [-1, 64, 64, 64]               0\n",
      "           Conv2d-18           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 64, 64]             128\n",
      "             ReLU-20           [-1, 64, 64, 64]               0\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "             ReLU-23           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-24           [-1, 64, 64, 64]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
      "             ReLU-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
      "           Conv2d-30          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-31          [-1, 128, 32, 32]             256\n",
      "             ReLU-32          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-35          [-1, 128, 32, 32]             256\n",
      "             ReLU-36          [-1, 128, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-40          [-1, 128, 32, 32]               0\n",
      "           Conv2d-41          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-42          [-1, 256, 16, 16]             512\n",
      "             ReLU-43          [-1, 256, 16, 16]               0\n",
      "           Conv2d-44          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
      "           Conv2d-46          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-47          [-1, 256, 16, 16]             512\n",
      "             ReLU-48          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-49          [-1, 256, 16, 16]               0\n",
      "           Conv2d-50          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-51          [-1, 256, 16, 16]             512\n",
      "             ReLU-52          [-1, 256, 16, 16]               0\n",
      "           Conv2d-53          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-54          [-1, 256, 16, 16]             512\n",
      "             ReLU-55          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-56          [-1, 256, 16, 16]               0\n",
      "           Conv2d-57            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-58            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-62            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-63            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-64            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-65            [-1, 512, 8, 8]               0\n",
      "           Conv2d-66            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-67            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-68            [-1, 512, 8, 8]               0\n",
      "           Conv2d-69            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-70            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-71            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-72            [-1, 512, 8, 8]               0\n",
      "           Conv2d-73            [-1, 512, 8, 8]         262,656\n",
      "             ReLU-74            [-1, 512, 8, 8]               0\n",
      "         Upsample-75          [-1, 512, 16, 16]               0\n",
      "           Conv2d-76          [-1, 256, 16, 16]          65,792\n",
      "             ReLU-77          [-1, 256, 16, 16]               0\n",
      "           Conv2d-78          [-1, 512, 16, 16]       3,539,456\n",
      "             ReLU-79          [-1, 512, 16, 16]               0\n",
      "         Upsample-80          [-1, 512, 32, 32]               0\n",
      "           Conv2d-81          [-1, 128, 32, 32]          16,512\n",
      "             ReLU-82          [-1, 128, 32, 32]               0\n",
      "           Conv2d-83          [-1, 256, 32, 32]       1,474,816\n",
      "             ReLU-84          [-1, 256, 32, 32]               0\n",
      "         Upsample-85          [-1, 256, 64, 64]               0\n",
      "           Conv2d-86           [-1, 64, 64, 64]           4,160\n",
      "             ReLU-87           [-1, 64, 64, 64]               0\n",
      "           Conv2d-88          [-1, 256, 64, 64]         737,536\n",
      "             ReLU-89          [-1, 256, 64, 64]               0\n",
      "         Upsample-90        [-1, 256, 128, 128]               0\n",
      "           Conv2d-91         [-1, 64, 128, 128]           4,160\n",
      "             ReLU-92         [-1, 64, 128, 128]               0\n",
      "           Conv2d-93        [-1, 128, 128, 128]         368,768\n",
      "             ReLU-94        [-1, 128, 128, 128]               0\n",
      "         Upsample-95        [-1, 128, 256, 256]               0\n",
      "           Conv2d-96         [-1, 64, 256, 256]         110,656\n",
      "             ReLU-97         [-1, 64, 256, 256]               0\n",
      "           Conv2d-98          [-1, 5, 256, 256]             325\n",
      "================================================================\n",
      "Total params: 17,800,075\n",
      "Trainable params: 17,800,075\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 466.00\n",
      "Params size (MB): 67.90\n",
      "Estimated Total Size (MB): 534.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(5)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from loss import dice_loss\n",
    "\n",
    "# Function that calculates the Jaccard overlap between two images for a given ROI\n",
    "def JaccardOverlap(first, second, region):\n",
    "    firstOther = np.zeros(first.shape)\n",
    "    secondOther = np.zeros(second.shape)\n",
    "    \n",
    "    firstOther[first == region] = 1\n",
    "    secondOther[second == region] = 1\n",
    "    \n",
    "    firstOther = firstOther.astype(bool)\n",
    "    secondOther = secondOther.astype(bool)\n",
    "    \n",
    "    \n",
    "    firstCount = np.sum(firstOther)\n",
    "    secondCount = np.sum(secondOther)\n",
    "    \n",
    "    \n",
    "    intersectionCount = np.sum(np.bitwise_and(firstOther.reshape(first.size), secondOther.reshape(second.size)))\n",
    "    \n",
    "    return intersectionCount / (firstCount + secondCount - intersectionCount)\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred.float(), target.float())\n",
    "        \n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float) # Changed this line\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 0/14\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d84e8934d976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c4e88edd498f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 5\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])\n",
    "print('Jaccard Overlap: ', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
