{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "# other_input_images = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "# other_target_masks = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "# print(np.array(input_images_rgb).shape)\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image, Right: Target mask\n",
    "# helper.plot_side_by_side([input_images, target_masks_rgb])\n",
    "x = target_masks[0, 2, :, :]\n",
    "something = plt.imshow(x)\n",
    "print(x)\n",
    "print(x.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355.0\n",
      "2324.0\n",
      "1933.0\n",
      "1712.0\n",
      "2475.0\n",
      "1753.0\n",
      "1846.0\n",
      "2789.0\n",
      "1361.0\n",
      "2581.0\n",
      "2808.0\n",
      "2904.0\n",
      "1341.0\n",
      "2545.0\n",
      "3010.0\n",
      "1933.0\n",
      "1672.0\n",
      "1868.0\n",
      "3116.0\n",
      "1789.0\n",
      "1599.0\n",
      "2218.0\n",
      "2426.0\n",
      "2599.0\n",
      "2728.0\n",
      "2216.0\n",
      "1284.0\n",
      "2265.0\n",
      "2342.0\n",
      "1823.0\n",
      "1471.0\n",
      "2761.0\n",
      "3291.0\n",
      "2850.0\n",
      "2164.0\n",
      "1647.0\n",
      "2340.0\n",
      "2174.0\n",
      "2193.0\n",
      "2964.0\n",
      "1452.0\n",
      "1644.0\n",
      "1665.0\n",
      "2919.0\n",
      "2563.0\n",
      "3280.0\n",
      "1544.0\n",
      "2893.0\n",
      "2686.0\n",
      "2270.0\n",
      "1458.0\n",
      "1274.0\n",
      "1576.0\n",
      "1859.0\n",
      "1660.0\n",
      "1800.0\n",
      "2882.0\n",
      "2458.0\n",
      "1423.0\n",
      "2412.0\n",
      "1667.0\n",
      "3045.0\n",
      "1584.0\n",
      "3003.0\n",
      "1854.0\n",
      "2472.0\n",
      "3379.0\n",
      "3037.0\n",
      "3258.0\n",
      "3118.0\n",
      "1852.0\n",
      "1798.0\n",
      "1481.0\n",
      "1631.0\n",
      "2568.0\n",
      "1545.0\n",
      "1387.0\n",
      "2020.0\n",
      "2668.0\n",
      "2249.0\n",
      "1657.0\n",
      "1791.0\n",
      "1350.0\n",
      "1843.0\n",
      "1412.0\n",
      "1487.0\n",
      "2652.0\n",
      "2744.0\n",
      "2818.0\n",
      "2786.0\n",
      "1994.0\n",
      "2722.0\n",
      "1959.0\n",
      "2545.0\n",
      "2452.0\n",
      "2243.0\n",
      "1458.0\n",
      "2192.0\n",
      "1602.0\n",
      "1615.0\n",
      "1781.0\n",
      "2143.0\n",
      "1370.0\n",
      "2180.0\n",
      "1824.0\n",
      "2284.0\n",
      "1407.0\n",
      "2228.0\n",
      "2575.0\n",
      "3015.0\n",
      "1410.0\n",
      "1859.0\n",
      "1901.0\n",
      "2680.0\n",
      "1992.0\n",
      "1791.0\n",
      "2517.0\n",
      "2508.0\n",
      "1771.0\n",
      "2577.0\n",
      "2138.0\n",
      "2554.0\n",
      "2471.0\n",
      "2624.0\n",
      "1634.0\n",
      "1992.0\n",
      "2614.0\n",
      "2110.0\n",
      "1390.0\n",
      "2523.0\n",
      "1567.0\n",
      "2268.0\n",
      "2662.0\n",
      "2481.0\n",
      "2271.0\n",
      "1323.0\n",
      "2756.0\n",
      "1517.0\n",
      "2547.0\n",
      "3305.0\n",
      "2945.0\n",
      "1561.0\n",
      "2234.0\n",
      "1310.0\n",
      "2957.0\n",
      "1780.0\n",
      "2446.0\n",
      "2459.0\n",
      "1864.0\n",
      "3183.0\n",
      "2435.0\n",
      "2435.0\n",
      "2333.0\n",
      "2099.0\n",
      "1642.0\n",
      "1542.0\n",
      "2227.0\n",
      "1898.0\n",
      "2070.0\n",
      "1845.0\n",
      "1684.0\n",
      "2692.0\n",
      "2869.0\n",
      "1635.0\n",
      "2242.0\n",
      "2759.0\n",
      "2056.0\n",
      "1433.0\n",
      "1756.0\n",
      "1685.0\n",
      "1433.0\n",
      "3004.0\n",
      "2020.0\n",
      "2913.0\n",
      "1738.0\n",
      "2047.0\n",
      "3351.0\n",
      "2055.0\n",
      "1775.0\n",
      "2577.0\n",
      "2350.0\n",
      "2634.0\n",
      "2264.0\n",
      "2493.0\n",
      "2747.0\n",
      "2239.0\n",
      "2197.0\n",
      "1725.0\n",
      "1876.0\n",
      "2198.0\n",
      "2646.0\n",
      "1779.0\n",
      "2202.0\n",
      "1818.0\n",
      "1339.0\n",
      "1970.0\n",
      "1818.0\n",
      "2389.0\n",
      "2281.0\n",
      "1700.0\n",
      "2387.0\n",
      "3242.0\n",
      "3329.0\n",
      "3511.0\n",
      "2080.0\n",
      "2819.0\n",
      "1975.0\n",
      "1494.0\n",
      "1830.0\n",
      "2971.0\n",
      "1994.0\n",
      "2131.0\n",
      "2081.0\n",
      "2527.0\n",
      "1405.0\n",
      "2378.0\n",
      "2999.0\n",
      "1860.0\n",
      "2244.0\n",
      "2024.0\n",
      "2098.0\n",
      "2806.0\n",
      "2087.0\n",
      "1333.0\n",
      "2456.0\n",
      "3170.0\n",
      "2128.0\n",
      "1820.0\n",
      "1887.0\n",
      "1564.0\n",
      "2310.0\n",
      "1804.0\n",
      "2694.0\n",
      "2389.0\n",
      "2321.0\n",
      "2398.0\n",
      "3183.0\n",
      "2484.0\n",
      "2087.0\n",
      "2938.0\n",
      "1631.0\n",
      "1951.0\n",
      "2795.0\n",
      "2199.0\n",
      "1579.0\n",
      "2797.0\n",
      "1816.0\n",
      "2122.0\n",
      "3198.0\n",
      "2764.0\n",
      "2799.0\n",
      "2489.0\n",
      "1731.0\n",
      "1313.0\n",
      "3468.0\n",
      "3522.0\n",
      "2537.0\n",
      "1734.0\n",
      "1646.0\n",
      "2009.0\n",
      "2362.0\n",
      "2972.0\n",
      "2004.0\n",
      "2782.0\n",
      "2688.0\n",
      "2420.0\n",
      "1546.0\n",
      "2633.0\n",
      "1600.0\n",
      "1517.0\n",
      "1901.0\n",
      "1555.0\n",
      "2188.0\n",
      "1443.0\n",
      "3450.0\n",
      "1389.0\n",
      "3207.0\n",
      "2572.0\n",
      "2419.0\n",
      "1880.0\n",
      "1736.0\n",
      "2598.0\n",
      "2245.0\n",
      "2698.0\n",
      "1666.0\n",
      "2835.0\n",
      "1789.0\n",
      "2355.0\n",
      "2312.0\n",
      "1526.0\n",
      "1723.0\n",
      "2129.0\n",
      "1637.0\n",
      "2781.0\n",
      "2914.0\n",
      "1862.0\n",
      "2220.0\n",
      "2807.0\n",
      "2220.0\n",
      "3290.0\n",
      "1767.0\n",
      "1505.0\n",
      "2580.0\n",
      "1942.0\n",
      "2299.0\n",
      "2755.0\n",
      "2141.0\n",
      "1924.0\n",
      "2441.0\n",
      "1578.0\n",
      "2684.0\n",
      "1259.0\n",
      "1796.0\n",
      "1573.0\n",
      "2217.0\n",
      "1511.0\n",
      "2774.0\n",
      "2264.0\n",
      "2004.0\n",
      "1385.0\n",
      "2123.0\n",
      "2866.0\n",
      "2320.0\n",
      "2691.0\n",
      "3071.0\n",
      "2069.0\n",
      "1566.0\n",
      "2199.0\n",
      "1799.0\n",
      "1478.0\n",
      "1918.0\n",
      "1753.0\n",
      "2779.0\n",
      "1426.0\n",
      "2276.0\n",
      "2831.0\n",
      "1523.0\n",
      "3523.0\n",
      "2359.0\n",
      "2461.0\n",
      "1563.0\n",
      "2609.0\n",
      "3219.0\n",
      "2081.0\n",
      "2039.0\n",
      "2197.0\n",
      "1588.0\n",
      "1453.0\n",
      "1326.0\n",
      "2294.0\n",
      "3379.0\n",
      "2246.0\n",
      "1580.0\n",
      "2861.0\n",
      "2152.0\n",
      "3145.0\n",
      "1932.0\n",
      "1785.0\n",
      "2614.0\n",
      "2508.0\n",
      "1859.0\n",
      "1598.0\n",
      "2482.0\n",
      "1681.0\n",
      "2594.0\n",
      "2510.0\n",
      "2332.0\n",
      "2062.0\n",
      "1635.0\n",
      "2536.0\n",
      "1922.0\n",
      "1431.0\n",
      "2770.0\n",
      "1718.0\n",
      "3001.0\n",
      "2130.0\n",
      "2072.0\n",
      "1653.0\n",
      "2603.0\n",
      "1882.0\n",
      "2715.0\n",
      "1835.0\n",
      "1685.0\n",
      "2487.0\n",
      "1768.0\n",
      "2111.0\n",
      "2303.0\n",
      "2366.0\n",
      "1456.0\n",
      "3517.0\n",
      "1777.0\n",
      "2504.0\n",
      "2490.0\n",
      "2290.0\n",
      "1327.0\n",
      "3351.0\n",
      "1524.0\n",
      "2055.0\n",
      "2564.0\n",
      "1884.0\n",
      "1869.0\n",
      "2089.0\n",
      "3320.0\n",
      "2812.0\n",
      "2221.0\n",
      "2526.0\n",
      "1992.0\n",
      "2705.0\n",
      "1494.0\n",
      "3432.0\n",
      "1773.0\n",
      "1879.0\n",
      "2360.0\n",
      "2777.0\n",
      "2541.0\n",
      "1788.0\n",
      "2401.0\n",
      "2144.0\n",
      "1848.0\n",
      "2807.0\n",
      "1858.0\n",
      "1401.0\n",
      "2580.0\n",
      "1421.0\n",
      "3502.0\n",
      "1619.0\n",
      "2089.0\n",
      "1741.0\n",
      "2689.0\n",
      "1674.0\n",
      "2428.0\n",
      "1676.0\n",
      "3276.0\n",
      "2239.0\n",
      "2159.0\n",
      "2321.0\n",
      "1639.0\n",
      "2084.0\n",
      "2483.0\n",
      "1404.0\n",
      "3227.0\n",
      "2882.0\n",
      "2190.0\n",
      "1879.0\n",
      "2586.0\n",
      "2483.0\n",
      "2795.0\n",
      "2041.0\n",
      "3021.0\n",
      "1722.0\n",
      "2205.0\n",
      "3494.0\n",
      "2440.0\n",
      "2607.0\n",
      "2076.0\n",
      "2187.0\n",
      "1769.0\n",
      "2385.0\n",
      "1562.0\n",
      "1961.0\n",
      "1604.0\n",
      "2531.0\n",
      "1641.0\n",
      "2137.0\n",
      "1693.0\n",
      "1830.0\n",
      "1448.0\n",
      "2690.0\n",
      "1636.0\n",
      "2306.0\n",
      "1799.0\n",
      "2276.0\n",
      "1455.0\n",
      "2463.0\n",
      "1324.0\n",
      "2367.0\n",
      "2752.0\n",
      "1511.0\n",
      "2700.0\n",
      "2427.0\n",
      "2576.0\n",
      "2781.0\n",
      "3155.0\n",
      "3345.0\n",
      "2509.0\n",
      "1494.0\n",
      "1641.0\n",
      "1933.0\n",
      "1502.0\n",
      "2790.0\n",
      "1839.0\n",
      "1861.0\n",
      "1907.0\n",
      "2352.0\n",
      "2700.0\n",
      "2460.0\n",
      "2573.0\n",
      "1526.0\n",
      "1851.0\n",
      "2392.0\n",
      "2638.0\n",
      "1860.0\n",
      "1438.0\n",
      "2577.0\n",
      "2240.0\n",
      "3121.0\n",
      "2823.0\n",
      "2326.0\n",
      "1911.0\n",
      "2153.0\n",
      "1963.0\n",
      "1429.0\n",
      "2591.0\n",
      "2738.0\n",
      "2247.0\n",
      "2137.0\n",
      "2084.0\n",
      "2363.0\n",
      "2273.0\n",
      "2099.0\n",
      "2733.0\n",
      "2897.0\n",
      "1496.0\n",
      "1907.0\n",
      "2983.0\n",
      "1796.0\n",
      "1857.0\n",
      "2333.0\n",
      "3211.0\n",
      "2422.0\n",
      "1563.0\n",
      "1750.0\n",
      "3200.0\n",
      "2352.0\n",
      "2222.0\n",
      "1746.0\n",
      "1627.0\n",
      "2182.0\n",
      "2912.0\n",
      "1547.0\n",
      "2199.0\n",
      "1994.0\n",
      "3166.0\n",
      "2334.0\n",
      "2522.0\n",
      "1881.0\n",
      "2449.0\n",
      "1639.0\n",
      "1876.0\n",
      "2197.0\n",
      "2538.0\n",
      "2790.0\n",
      "1504.0\n",
      "1540.0\n",
      "1899.0\n",
      "2438.0\n",
      "1726.0\n",
      "2241.0\n",
      "1413.0\n",
      "2225.0\n",
      "2899.0\n",
      "2394.0\n",
      "1631.0\n",
      "2194.0\n",
      "2133.0\n",
      "2510.0\n",
      "2226.0\n",
      "2731.0\n",
      "1793.0\n",
      "1266.0\n",
      "2194.0\n",
      "2233.0\n",
      "3113.0\n",
      "2768.0\n",
      "2137.0\n",
      "1359.0\n",
      "2652.0\n",
      "2814.0\n",
      "2606.0\n",
      "2439.0\n",
      "2618.0\n",
      "2595.0\n",
      "1707.0\n",
      "2702.0\n",
      "2731.0\n",
      "1779.0\n",
      "2901.0\n",
      "3252.0\n",
      "2188.0\n",
      "2562.0\n",
      "2502.0\n",
      "1357.0\n",
      "2566.0\n",
      "1916.0\n",
      "1592.0\n",
      "2146.0\n",
      "1650.0\n",
      "1703.0\n",
      "1462.0\n",
      "2148.0\n",
      "3007.0\n",
      "1703.0\n",
      "3207.0\n",
      "1692.0\n",
      "1573.0\n",
      "1306.0\n",
      "1630.0\n",
      "1513.0\n",
      "2407.0\n",
      "1983.0\n",
      "2208.0\n",
      "3069.0\n",
      "2551.0\n",
      "1515.0\n",
      "2969.0\n",
      "2173.0\n",
      "2488.0\n",
      "1665.0\n",
      "1768.0\n",
      "2826.0\n",
      "1924.0\n",
      "1612.0\n",
      "2198.0\n",
      "1364.0\n",
      "1763.0\n",
      "1540.0\n",
      "3265.0\n",
      "2709.0\n",
      "2537.0\n",
      "2255.0\n",
      "1875.0\n",
      "1312.0\n",
      "2399.0\n",
      "2561.0\n",
      "1440.0\n",
      "1683.0\n",
      "1639.0\n",
      "2991.0\n",
      "1688.0\n",
      "2423.0\n",
      "2567.0\n",
      "2996.0\n",
      "2211.0\n",
      "2746.0\n",
      "1718.0\n",
      "1442.0\n",
      "1969.0\n",
      "2079.0\n",
      "2716.0\n",
      "1926.0\n",
      "2690.0\n",
      "2616.0\n",
      "2683.0\n",
      "1269.0\n",
      "2312.0\n",
      "3487.0\n",
      "1580.0\n",
      "2797.0\n",
      "1861.0\n",
      "1273.0\n",
      "1540.0\n",
      "1854.0\n",
      "2626.0\n",
      "1715.0\n",
      "2698.0\n",
      "1860.0\n",
      "2204.0\n",
      "2660.0\n",
      "1848.0\n",
      "1973.0\n",
      "2954.0\n",
      "2936.0\n",
      "1600.0\n",
      "2214.0\n",
      "1588.0\n",
      "2813.0\n",
      "1479.0\n",
      "1353.0\n",
      "1683.0\n",
      "2127.0\n",
      "2490.0\n",
      "2655.0\n",
      "2206.0\n",
      "1921.0\n",
      "1914.0\n",
      "1962.0\n",
      "2527.0\n",
      "2076.0\n",
      "1560.0\n",
      "1736.0\n",
      "1751.0\n",
      "2076.0\n",
      "1782.0\n",
      "2782.0\n",
      "1805.0\n",
      "2029.0\n",
      "1841.0\n",
      "2708.0\n",
      "1299.0\n",
      "2760.0\n",
      "2092.0\n",
      "1431.0\n",
      "2666.0\n",
      "1958.0\n",
      "1512.0\n",
      "1735.0\n",
      "2361.0\n",
      "1577.0\n",
      "2173.0\n",
      "3189.0\n",
      "2825.0\n",
      "2130.0\n",
      "1352.0\n",
      "1853.0\n",
      "2353.0\n",
      "2011.0\n",
      "2939.0\n",
      "2713.0\n",
      "2686.0\n",
      "1992.0\n",
      "1975.0\n",
      "2970.0\n",
      "2459.0\n",
      "2402.0\n",
      "2294.0\n",
      "2946.0\n",
      "2737.0\n",
      "2781.0\n",
      "2729.0\n",
      "2079.0\n",
      "2879.0\n",
      "2686.0\n",
      "2224.0\n",
      "1748.0\n",
      "2684.0\n",
      "1901.0\n",
      "1546.0\n",
      "2785.0\n",
      "3163.0\n",
      "2644.0\n",
      "2410.0\n",
      "2939.0\n",
      "2802.0\n",
      "2682.0\n",
      "2661.0\n",
      "2082.0\n",
      "1741.0\n",
      "1448.0\n",
      "1989.0\n",
      "1581.0\n",
      "2373.0\n",
      "1377.0\n",
      "1552.0\n",
      "2360.0\n",
      "2972.0\n",
      "1869.0\n",
      "2409.0\n",
      "2063.0\n",
      "1609.0\n",
      "1367.0\n",
      "1478.0\n",
      "1713.0\n",
      "2529.0\n",
      "1634.0\n",
      "1863.0\n",
      "2996.0\n",
      "2068.0\n",
      "1831.0\n",
      "2770.0\n",
      "2138.0\n",
      "2498.0\n",
      "2070.0\n",
      "1504.0\n",
      "1930.0\n",
      "1624.0\n",
      "2259.0\n",
      "1446.0\n",
      "2233.0\n",
      "1592.0\n",
      "2310.0\n",
      "2852.0\n",
      "2081.0\n",
      "2479.0\n",
      "1607.0\n",
      "2534.0\n",
      "2964.0\n",
      "2581.0\n",
      "2558.0\n",
      "2428.0\n",
      "3102.0\n",
      "1770.0\n",
      "1756.0\n",
      "2964.0\n",
      "1765.0\n",
      "1860.0\n",
      "2084.0\n",
      "2722.0\n",
      "2103.0\n",
      "2085.0\n",
      "2052.0\n",
      "2598.0\n",
      "1664.0\n",
      "1571.0\n",
      "2387.0\n",
      "1677.0\n",
      "1323.0\n",
      "1558.0\n",
      "1539.0\n",
      "2439.0\n",
      "2518.0\n",
      "2211.0\n",
      "2356.0\n",
      "1703.0\n",
      "1610.0\n",
      "2086.0\n",
      "1973.0\n",
      "1635.0\n",
      "2040.0\n",
      "2221.0\n",
      "2190.0\n",
      "1645.0\n",
      "3478.0\n",
      "1558.0\n",
      "2008.0\n",
      "1556.0\n",
      "1958.0\n",
      "1641.0\n",
      "1425.0\n",
      "1916.0\n",
      "1613.0\n",
      "1641.0\n",
      "2147.0\n",
      "2484.0\n",
      "2797.0\n",
      "2058.0\n",
      "1414.0\n",
      "1556.0\n",
      "3448.0\n",
      "3429.0\n",
      "2562.0\n",
      "1796.0\n",
      "1550.0\n",
      "1470.0\n",
      "1896.0\n",
      "1925.0\n",
      "1914.0\n",
      "2878.0\n",
      "1453.0\n",
      "2624.0\n",
      "2028.0\n",
      "2229.0\n",
      "2456.0\n",
      "1831.0\n",
      "1796.0\n",
      "1713.0\n",
      "2453.0\n",
      "2360.0\n",
      "2836.0\n",
      "2471.0\n",
      "2301.0\n",
      "1740.0\n",
      "1629.0\n",
      "2192.0\n",
      "2075.0\n",
      "1794.0\n",
      "1796.0\n",
      "3324.0\n",
      "1791.0\n",
      "1891.0\n",
      "2116.0\n",
      "2012.0\n",
      "2939.0\n",
      "2403.0\n",
      "2923.0\n",
      "2786.0\n",
      "2061.0\n",
      "2570.0\n",
      "1399.0\n",
      "1448.0\n",
      "1312.0\n",
      "1342.0\n",
      "1391.0\n",
      "2182.0\n",
      "1712.0\n",
      "1864.0\n",
      "2545.0\n",
      "2085.0\n",
      "1600.0\n",
      "2242.0\n",
      "2566.0\n",
      "2059.0\n",
      "2462.0\n",
      "1861.0\n",
      "2395.0\n",
      "1698.0\n",
      "1799.0\n",
      "2311.0\n",
      "2555.0\n",
      "1400.0\n",
      "2229.0\n",
      "1588.0\n",
      "2228.0\n",
      "3514.0\n",
      "1622.0\n",
      "2905.0\n",
      "2425.0\n",
      "1850.0\n",
      "2209.0\n",
      "1901.0\n",
      "2463.0\n",
      "3360.0\n",
      "2385.0\n",
      "1425.0\n",
      "2430.0\n",
      "2436.0\n",
      "1540.0\n",
      "2905.0\n",
      "1656.0\n",
      "2203.0\n",
      "1785.0\n",
      "2703.0\n",
      "1656.0\n",
      "1831.0\n",
      "3420.0\n",
      "2484.0\n",
      "2703.0\n",
      "2217.0\n",
      "3182.0\n",
      "2852.0\n",
      "2705.0\n",
      "2966.0\n",
      "2498.0\n",
      "1937.0\n",
      "1749.0\n",
      "1499.0\n",
      "2186.0\n",
      "2413.0\n",
      "3078.0\n",
      "3285.0\n",
      "2913.0\n",
      "1980.0\n",
      "2550.0\n",
      "2074.0\n",
      "1384.0\n",
      "2866.0\n",
      "1678.0\n",
      "2320.0\n",
      "1646.0\n",
      "2845.0\n",
      "2474.0\n",
      "1910.0\n",
      "2530.0\n",
      "2428.0\n",
      "1913.0\n",
      "2580.0\n",
      "2920.0\n",
      "2818.0\n",
      "1493.0\n",
      "2578.0\n",
      "2428.0\n",
      "2429.0\n",
      "2352.0\n",
      "1517.0\n",
      "1900.0\n",
      "1628.0\n",
      "1788.0\n",
      "1957.0\n",
      "2092.0\n",
      "2921.0\n",
      "1813.0\n",
      "2238.0\n",
      "2721.0\n",
      "2639.0\n",
      "1442.0\n",
      "1786.0\n",
      "2693.0\n",
      "2879.0\n",
      "1707.0\n",
      "3496.0\n",
      "2631.0\n",
      "2268.0\n",
      "1994.0\n",
      "2786.0\n",
      "3554.0\n",
      "1962.0\n",
      "1445.0\n",
      "1876.0\n",
      "1415.0\n",
      "2393.0\n",
      "2099.0\n",
      "1903.0\n",
      "2354.0\n",
      "1303.0\n",
      "2083.0\n",
      "1551.0\n",
      "2796.0\n",
      "1627.0\n",
      "2057.0\n",
      "1794.0\n",
      "2690.0\n",
      "1445.0\n",
      "2384.0\n",
      "2132.0\n",
      "2735.0\n",
      "1709.0\n",
      "2224.0\n",
      "2830.0\n",
      "3109.0\n",
      "1803.0\n",
      "2092.0\n",
      "2427.0\n",
      "1527.0\n",
      "3336.0\n",
      "1431.0\n",
      "2668.0\n",
      "2127.0\n",
      "2788.0\n",
      "1797.0\n",
      "2553.0\n",
      "2084.0\n",
      "1909.0\n",
      "2630.0\n",
      "1800.0\n",
      "1992.0\n",
      "3352.0\n",
      "1555.0\n",
      "1490.0\n",
      "1437.0\n",
      "1469.0\n",
      "3151.0\n",
      "3431.0\n",
      "1601.0\n",
      "1945.0\n",
      "2207.0\n",
      "1814.0\n",
      "1819.0\n",
      "1296.0\n",
      "2800.0\n",
      "1995.0\n",
      "1576.0\n",
      "1983.0\n",
      "2580.0\n",
      "1813.0\n",
      "2793.0\n",
      "2440.0\n",
      "2363.0\n",
      "1502.0\n",
      "2335.0\n",
      "2512.0\n",
      "1846.0\n",
      "2100.0\n",
      "2354.0\n",
      "1334.0\n",
      "2292.0\n",
      "2283.0\n",
      "3033.0\n",
      "2828.0\n",
      "3230.0\n",
      "2749.0\n",
      "1621.0\n",
      "2557.0\n",
      "1635.0\n",
      "1428.0\n",
      "1955.0\n",
      "2591.0\n",
      "2171.0\n",
      "2335.0\n",
      "3357.0\n",
      "1515.0\n",
      "2944.0\n",
      "3245.0\n",
      "2816.0\n",
      "2768.0\n",
      "2328.0\n",
      "2156.0\n",
      "1374.0\n",
      "2046.0\n",
      "1873.0\n",
      "2397.0\n",
      "1853.0\n",
      "2556.0\n",
      "1862.0\n",
      "2078.0\n",
      "1515.0\n",
      "1917.0\n",
      "2292.0\n",
      "2195.0\n",
      "2568.0\n",
      "2098.0\n",
      "1628.0\n",
      "2203.0\n",
      "1868.0\n",
      "2153.0\n",
      "1641.0\n",
      "2529.0\n",
      "2816.0\n",
      "2930.0\n",
      "2586.0\n",
      "1728.0\n",
      "1379.0\n",
      "1329.0\n",
      "1924.0\n",
      "2361.0\n",
      "2272.0\n",
      "1795.0\n",
      "2467.0\n",
      "2125.0\n",
      "1618.0\n",
      "2150.0\n",
      "3239.0\n",
      "1993.0\n",
      "1860.0\n",
      "2006.0\n",
      "1509.0\n",
      "2257.0\n",
      "2964.0\n",
      "1339.0\n",
      "1414.0\n",
      "2932.0\n",
      "1356.0\n",
      "2461.0\n",
      "2510.0\n",
      "2862.0\n",
      "1329.0\n",
      "1535.0\n",
      "2571.0\n",
      "1816.0\n",
      "3503.0\n",
      "1538.0\n",
      "3206.0\n",
      "2504.0\n",
      "1528.0\n",
      "2388.0\n",
      "1343.0\n",
      "1715.0\n",
      "2192.0\n",
      "2126.0\n",
      "1306.0\n",
      "2890.0\n",
      "1449.0\n",
      "2693.0\n",
      "1948.0\n",
      "2816.0\n",
      "2206.0\n",
      "2056.0\n",
      "2105.0\n",
      "1829.0\n",
      "2360.0\n",
      "1851.0\n",
      "1763.0\n",
      "2283.0\n",
      "1702.0\n",
      "2220.0\n",
      "1400.0\n",
      "2448.0\n",
      "2936.0\n",
      "2716.0\n",
      "2177.0\n",
      "2059.0\n",
      "2050.0\n",
      "2123.0\n",
      "1906.0\n",
      "1493.0\n",
      "1945.0\n",
      "2754.0\n",
      "1960.0\n",
      "2653.0\n",
      "3141.0\n",
      "2060.0\n",
      "1431.0\n",
      "1533.0\n",
      "2206.0\n",
      "2548.0\n",
      "1546.0\n",
      "1687.0\n",
      "2828.0\n",
      "2903.0\n",
      "1382.0\n",
      "2471.0\n",
      "2280.0\n",
      "2436.0\n",
      "3381.0\n",
      "1562.0\n",
      "2719.0\n",
      "1781.0\n",
      "2370.0\n",
      "1556.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800.0\n",
      "3153.0\n",
      "1380.0\n",
      "2422.0\n",
      "1439.0\n",
      "2457.0\n",
      "2702.0\n",
      "1637.0\n",
      "3283.0\n",
      "2248.0\n",
      "2797.0\n",
      "2178.0\n",
      "2087.0\n",
      "1357.0\n",
      "2548.0\n",
      "1893.0\n",
      "2057.0\n",
      "2242.0\n",
      "2851.0\n",
      "1913.0\n",
      "2457.0\n",
      "2319.0\n",
      "2058.0\n",
      "3164.0\n",
      "3000.0\n",
      "2434.0\n",
      "2945.0\n",
      "1367.0\n",
      "2010.0\n",
      "1997.0\n",
      "1939.0\n",
      "3052.0\n",
      "2535.0\n",
      "2853.0\n",
      "2575.0\n",
      "1752.0\n",
      "2761.0\n",
      "1384.0\n",
      "2496.0\n",
      "2282.0\n",
      "1944.0\n",
      "1842.0\n",
      "2460.0\n",
      "1634.0\n",
      "2018.0\n",
      "2713.0\n",
      "3149.0\n",
      "3165.0\n",
      "1976.0\n",
      "2308.0\n",
      "2452.0\n",
      "1808.0\n",
      "2051.0\n",
      "2504.0\n",
      "2670.0\n",
      "2764.0\n",
      "1671.0\n",
      "1649.0\n",
      "2589.0\n",
      "1400.0\n",
      "2850.0\n",
      "1339.0\n",
      "3285.0\n",
      "1922.0\n",
      "2725.0\n",
      "1614.0\n",
      "1779.0\n",
      "2483.0\n",
      "2237.0\n",
      "2760.0\n",
      "1822.0\n",
      "2931.0\n",
      "3318.0\n",
      "2679.0\n",
      "1315.0\n",
      "2293.0\n",
      "2881.0\n",
      "2052.0\n",
      "1611.0\n",
      "1761.0\n",
      "2776.0\n",
      "1507.0\n",
      "1724.0\n",
      "2634.0\n",
      "1503.0\n",
      "2136.0\n",
      "1654.0\n",
      "2104.0\n",
      "2365.0\n",
      "2642.0\n",
      "1764.0\n",
      "1546.0\n",
      "1737.0\n",
      "2968.0\n",
      "2131.0\n",
      "2868.0\n",
      "2099.0\n",
      "3009.0\n",
      "1869.0\n",
      "3031.0\n",
      "1992.0\n",
      "2820.0\n",
      "1752.0\n",
      "1620.0\n",
      "1745.0\n",
      "2176.0\n",
      "2921.0\n",
      "1713.0\n",
      "2526.0\n",
      "3147.0\n",
      "1564.0\n",
      "2666.0\n",
      "2200.0\n",
      "2261.0\n",
      "2856.0\n",
      "2826.0\n",
      "2753.0\n",
      "2801.0\n",
      "2883.0\n",
      "2315.0\n",
      "2459.0\n",
      "1524.0\n",
      "2670.0\n",
      "2661.0\n",
      "2459.0\n",
      "2240.0\n",
      "1951.0\n",
      "2290.0\n",
      "2181.0\n",
      "1754.0\n",
      "2879.0\n",
      "1912.0\n",
      "2078.0\n",
      "1814.0\n",
      "1976.0\n",
      "2721.0\n",
      "2298.0\n",
      "1721.0\n",
      "1872.0\n",
      "1381.0\n",
      "1539.0\n",
      "2538.0\n",
      "2307.0\n",
      "1959.0\n",
      "2451.0\n",
      "2912.0\n",
      "2312.0\n",
      "2970.0\n",
      "2836.0\n",
      "2528.0\n",
      "1438.0\n",
      "2398.0\n",
      "2037.0\n",
      "2906.0\n",
      "2582.0\n",
      "2824.0\n",
      "3542.0\n",
      "2347.0\n",
      "2203.0\n",
      "1981.0\n",
      "2682.0\n",
      "2837.0\n",
      "3240.0\n",
      "2236.0\n",
      "1798.0\n",
      "3162.0\n",
      "2757.0\n",
      "2690.0\n",
      "1346.0\n",
      "1403.0\n",
      "1699.0\n",
      "2292.0\n",
      "1433.0\n",
      "1557.0\n",
      "2451.0\n",
      "1897.0\n",
      "2239.0\n",
      "1665.0\n",
      "2637.0\n",
      "2083.0\n",
      "2357.0\n",
      "2299.0\n",
      "2139.0\n",
      "2805.0\n",
      "1969.0\n",
      "1963.0\n",
      "2435.0\n",
      "2424.0\n",
      "1588.0\n",
      "2083.0\n",
      "2129.0\n",
      "2191.0\n",
      "3328.0\n",
      "2440.0\n",
      "2575.0\n",
      "2295.0\n",
      "1801.0\n",
      "1914.0\n",
      "1610.0\n",
      "1718.0\n",
      "2955.0\n",
      "1994.0\n",
      "1680.0\n",
      "2552.0\n",
      "2270.0\n",
      "2548.0\n",
      "1899.0\n",
      "3174.0\n",
      "1774.0\n",
      "2628.0\n",
      "3235.0\n",
      "1534.0\n",
      "2139.0\n",
      "3174.0\n",
      "1896.0\n",
      "2320.0\n",
      "1679.0\n",
      "2805.0\n",
      "2789.0\n",
      "1570.0\n",
      "1933.0\n",
      "1906.0\n",
      "1352.0\n",
      "1767.0\n",
      "1817.0\n",
      "2566.0\n",
      "1618.0\n",
      "3289.0\n",
      "2155.0\n",
      "2657.0\n",
      "2791.0\n",
      "2920.0\n",
      "2652.0\n",
      "2268.0\n",
      "2645.0\n",
      "2781.0\n",
      "1616.0\n",
      "2450.0\n",
      "1765.0\n",
      "2285.0\n",
      "2823.0\n",
      "1390.0\n",
      "2130.0\n",
      "2375.0\n",
      "1661.0\n",
      "1747.0\n",
      "2670.0\n",
      "2209.0\n",
      "2827.0\n",
      "3293.0\n",
      "2210.0\n",
      "2453.0\n",
      "1615.0\n",
      "1907.0\n",
      "1459.0\n",
      "1915.0\n",
      "2438.0\n",
      "2660.0\n",
      "2359.0\n",
      "2744.0\n",
      "1617.0\n",
      "2210.0\n",
      "2508.0\n",
      "2032.0\n",
      "2237.0\n",
      "2462.0\n",
      "2083.0\n",
      "2649.0\n",
      "2487.0\n",
      "2488.0\n",
      "1433.0\n",
      "2409.0\n",
      "1790.0\n",
      "3064.0\n",
      "2232.0\n",
      "1364.0\n",
      "1950.0\n",
      "2253.0\n",
      "1675.0\n",
      "2027.0\n",
      "1548.0\n",
      "1305.0\n",
      "1394.0\n",
      "2046.0\n",
      "1830.0\n",
      "2729.0\n",
      "1862.0\n",
      "2488.0\n",
      "2446.0\n",
      "2610.0\n",
      "1504.0\n",
      "2575.0\n",
      "2415.0\n",
      "1782.0\n",
      "2286.0\n",
      "2741.0\n",
      "1516.0\n",
      "2129.0\n",
      "2783.0\n",
      "1650.0\n",
      "2154.0\n",
      "1644.0\n",
      "2395.0\n",
      "2636.0\n",
      "2494.0\n",
      "2417.0\n",
      "2920.0\n",
      "1848.0\n",
      "2434.0\n",
      "1686.0\n",
      "1699.0\n",
      "1502.0\n",
      "1934.0\n",
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def makeSeg(image):\n",
    "    # print(image.min())\n",
    "    # print(image.max())\n",
    "    # image = image.reshape((1, image.shape[0], image.shape[1]))\n",
    "    # print(image)\n",
    "    # return\n",
    "    foreground = np.zeros(image.shape)\n",
    "    first = np.zeros(image.shape)\n",
    "    first[image == 2] = 1\n",
    "    print(np.sum(first))\n",
    "    foreground[image == 2] = 1\n",
    "    second = np.zeros(image.shape)\n",
    "    second[image == 3] = 1\n",
    "    foreground[image == 3] = 1\n",
    "    third = np.zeros(image.shape)\n",
    "    third[image == 41] = 1\n",
    "    foreground[image == 41] = 1\n",
    "    fourth = np.zeros(image.shape)\n",
    "    fourth[image == 42] = 1\n",
    "    foreground[image == 42] = 1\n",
    "    fifth = np.zeros(image.shape)\n",
    "    fifth[foreground == 0] = 1\n",
    "    \n",
    "    return np.concatenate((first.reshape(1, image.shape[0], image.shape[1]), \n",
    "                           second.reshape(1, image.shape[0], image.shape[1]), \n",
    "                           third.reshape(1, image.shape[0], image.shape[1]), \n",
    "                           fourth.reshape(1, image.shape[0], image.shape[1]), \n",
    "                           fifth.reshape(1, image.shape[0], image.shape[1])), axis=0)\n",
    "\n",
    "dirs = './segmented_images/'\n",
    "files = os.listdir(dirs)\n",
    "# print(files[0])\n",
    "segmented_images = np.array([makeSeg(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "# print(segmented_images[0].max())\n",
    "# print(segmented_images[0].min())\n",
    "# print(segmented_images[0][80 : -80, 80 : -80])\n",
    "x = segmented_images[0][2]\n",
    "print(x.max())\n",
    "print(x.min())\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "# dirs = '/A/'\n",
    "# files = os.listdir(dirs)\n",
    "# data = [imread(files[i]) for i in range(len(files))]\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms, datasets, models\n",
    "\n",
    "# class SimDataset(Dataset):\n",
    "#     def __init__(self, count, transform=None):\n",
    "#         self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)        \n",
    "#         self.transform = transform\n",
    "#         print(self.input_images.shape)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.input_images)\n",
    "    \n",
    "#     def __getitem__(self, idx):        \n",
    "#         image = self.input_images[idx]\n",
    "#         mask = self.target_masks[idx]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         return [image, mask]\n",
    "\n",
    "# # use same transform for train/val for this example\n",
    "# trans = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "# ])\n",
    "\n",
    "# train_set = SimDataset(2000, transform=trans)\n",
    "# val_set = SimDataset(200, transform=trans)\n",
    "\n",
    "# image_datasets = {\n",
    "#     'train': train_set, 'val': val_set\n",
    "# }\n",
    "\n",
    "# batch_size = 25\n",
    "\n",
    "# dataloaders = {\n",
    "#     'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "#     'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# }\n",
    "\n",
    "# dataset_sizes = {\n",
    "#     x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "# }\n",
    "\n",
    "# dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 1500, 'val': 500}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function that reshapes the input image to have 1 as the number of channels\n",
    "def makeInput(image):\n",
    "    # print(image.shape)\n",
    "    return image.reshape((image.shape[0], image.shape[1], 1))\n",
    "\n",
    "# ROIList = [2, 3, 41, 42]\n",
    "def makeSeg(image):\n",
    "    \n",
    "    image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "    foreground = np.zeros(image.shape)\n",
    "    first = np.zeros(image.shape)\n",
    "    first[image == 2] = 1\n",
    "    foreground[image == 2] = 1\n",
    "    second = np.zeros(image.shape)\n",
    "    second[image == 3] = 1\n",
    "    foreground[image == 3] = 1\n",
    "    third = np.zeros(image.shape)\n",
    "    third[image == 41] = 1\n",
    "    foreground[image == 41] = 1\n",
    "    fourth = np.zeros(image.shape)\n",
    "    fourth[image == 42] = 1\n",
    "    foreground[image == 42] = 1\n",
    "    fifth = np.zeros(image.shape)\n",
    "    fifth[foreground == 0] = 1\n",
    "    \n",
    "    return np.concatenate((first, second, third, fourth, fifth), axis=2)\n",
    "    \n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        if train:\n",
    "            dirs = './original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            # print(files[0])\n",
    "            self.input_images = np.array([makeInput(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            # print(self.input_images.shape)\n",
    "            dirs = './segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        else:\n",
    "            dirs = './validation_original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.input_images = np.array([makeInput(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            dirs = './validation_segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            transformation = self.transform\n",
    "            image = transformation(image)\n",
    "            mask = transformation(mask)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Resize(192), # Added this line\n",
    "    # transforms.RandomCrop(180), # Added this line\n",
    "    # transforms.RandomRotation(10), # Added this line\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(train=True, transform=trans)\n",
    "val_set = SimDataset(train=False, transform=trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 1, 256, 256]) torch.Size([25, 5, 256, 256])\n",
      "-2.1179039301310043 2.2489082969432315 -1.6812636892111823 0.758381808876841\n",
      "-2.1179039301310043 2.6399999999999997 -0.9240885842478599 1.3201657692258186\n"
     ]
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "# plt.imshow(reverse_transform(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AvgPool2d(kernel_size=7, stride=1, padding=0),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "    \n",
    "list(base_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "        AvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 107.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "summary(base_model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layersi = convrelu(1, 3,1,0)\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        layersi = self.layersi(input)\n",
    "        x_original = self.conv_original_size0(layersi)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(layersi)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajay/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]               6\n",
      "              ReLU-2          [-1, 3, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]           1,792\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "            Conv2d-5         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "            Conv2d-7         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-8         [-1, 64, 128, 128]             128\n",
      "              ReLU-9         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-10           [-1, 64, 64, 64]               0\n",
      "           Conv2d-11           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 64, 64]             128\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "           Conv2d-14           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 64, 64]             128\n",
      "             ReLU-16           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-17           [-1, 64, 64, 64]               0\n",
      "           Conv2d-18           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 64, 64]             128\n",
      "             ReLU-20           [-1, 64, 64, 64]               0\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "             ReLU-23           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-24           [-1, 64, 64, 64]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
      "             ReLU-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
      "           Conv2d-30          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-31          [-1, 128, 32, 32]             256\n",
      "             ReLU-32          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-35          [-1, 128, 32, 32]             256\n",
      "             ReLU-36          [-1, 128, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-40          [-1, 128, 32, 32]               0\n",
      "           Conv2d-41          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-42          [-1, 256, 16, 16]             512\n",
      "             ReLU-43          [-1, 256, 16, 16]               0\n",
      "           Conv2d-44          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
      "           Conv2d-46          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-47          [-1, 256, 16, 16]             512\n",
      "             ReLU-48          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-49          [-1, 256, 16, 16]               0\n",
      "           Conv2d-50          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-51          [-1, 256, 16, 16]             512\n",
      "             ReLU-52          [-1, 256, 16, 16]               0\n",
      "           Conv2d-53          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-54          [-1, 256, 16, 16]             512\n",
      "             ReLU-55          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-56          [-1, 256, 16, 16]               0\n",
      "           Conv2d-57            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-58            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-62            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-63            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-64            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-65            [-1, 512, 8, 8]               0\n",
      "           Conv2d-66            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-67            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-68            [-1, 512, 8, 8]               0\n",
      "           Conv2d-69            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-70            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-71            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-72            [-1, 512, 8, 8]               0\n",
      "           Conv2d-73            [-1, 512, 8, 8]         262,656\n",
      "             ReLU-74            [-1, 512, 8, 8]               0\n",
      "         Upsample-75          [-1, 512, 16, 16]               0\n",
      "           Conv2d-76          [-1, 256, 16, 16]          65,792\n",
      "             ReLU-77          [-1, 256, 16, 16]               0\n",
      "           Conv2d-78          [-1, 512, 16, 16]       3,539,456\n",
      "             ReLU-79          [-1, 512, 16, 16]               0\n",
      "         Upsample-80          [-1, 512, 32, 32]               0\n",
      "           Conv2d-81          [-1, 128, 32, 32]          16,512\n",
      "             ReLU-82          [-1, 128, 32, 32]               0\n",
      "           Conv2d-83          [-1, 256, 32, 32]       1,474,816\n",
      "             ReLU-84          [-1, 256, 32, 32]               0\n",
      "         Upsample-85          [-1, 256, 64, 64]               0\n",
      "           Conv2d-86           [-1, 64, 64, 64]           4,160\n",
      "             ReLU-87           [-1, 64, 64, 64]               0\n",
      "           Conv2d-88          [-1, 256, 64, 64]         737,536\n",
      "             ReLU-89          [-1, 256, 64, 64]               0\n",
      "         Upsample-90        [-1, 256, 128, 128]               0\n",
      "           Conv2d-91         [-1, 64, 128, 128]           4,160\n",
      "             ReLU-92         [-1, 64, 128, 128]               0\n",
      "           Conv2d-93        [-1, 128, 128, 128]         368,768\n",
      "             ReLU-94        [-1, 128, 128, 128]               0\n",
      "         Upsample-95        [-1, 128, 256, 256]               0\n",
      "           Conv2d-96         [-1, 64, 256, 256]         110,656\n",
      "             ReLU-97         [-1, 64, 256, 256]               0\n",
      "           Conv2d-98          [-1, 5, 256, 256]             325\n",
      "================================================================\n",
      "Total params: 17,800,075\n",
      "Trainable params: 17,800,075\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 466.00\n",
      "Params size (MB): 67.90\n",
      "Estimated Total Size (MB): 534.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(5)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from loss import dice_loss\n",
    "\n",
    "# Function that calculates the Jaccard overlap between two images for a given ROI\n",
    "def JaccardOverlap(first, second, region):\n",
    "    firstOther = np.zeros(first.shape)\n",
    "    secondOther = np.zeros(second.shape)\n",
    "    \n",
    "    firstOther[first == region] = 1\n",
    "    secondOther[second == region] = 1\n",
    "    \n",
    "    firstOther = firstOther.astype(bool)\n",
    "    secondOther = secondOther.astype(bool)\n",
    "    \n",
    "    \n",
    "    firstCount = np.sum(firstOther)\n",
    "    secondCount = np.sum(secondOther)\n",
    "    \n",
    "    \n",
    "    intersectionCount = np.sum(np.bitwise_and(firstOther.reshape(first.size), secondOther.reshape(second.size)))\n",
    "    \n",
    "    return intersectionCount / (firstCount + secondCount - intersectionCount)\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred.float(), target.float())\n",
    "        \n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float) # Changed this line\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 0/14\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d84e8934d976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-c4e88edd498f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0c81a5bfa1cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mlayer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mlayer4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 5\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])\n",
    "print('Jaccard Overlap: ', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
