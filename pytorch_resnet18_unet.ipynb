{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADa1JREFUeJzt3WuoZfV5x/Hvr17RKmq9YNXUC5OAlvbEDhoQxdQmXigZLZjOUJKplY6CQkv7oppCI30V2lghtDEoGRwhaqzW6otp1AwlUqiNM8nUeI2jMXqcYUw0GKnBZManL/Y6ZP9nzsk5nn075/j9wGGv9d9r7/X8Zw8/1lp7sZ9UFZI049cmXYCkpcVQkNQwFCQ1DAVJDUNBUsNQkNQYWSgkuSTJ80l2JLlhVPuRNFwZxX0KSQ4Avg98ApgGngDWVdUzQ9+ZpKEa1ZHCOcCOqnqpqn4O3AOsGdG+JA3RgSN635OAV/vWp4Fz59r44BxSh3L4iEqRBPA2P/lxVR0333ajCoXMMtacpyTZAGwAOJTDODcXjagUSQDfrPt+uJDtRnX6MA2c0rd+MrCzf4Oquq2qVlfV6oM4ZERlSHq/RhUKTwCrkpyW5GBgLfDQiPYlaYhGcvpQVXuSXA88DBwAbKyqp0exL0nDNaprClTVZmDzqN5f0mh4R6OkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIahoKkhqEgqWEoSGoYCpIaiw6FJKck+c8kzyZ5OslfdOM3JXktyfbu77LhlStp1Ab5Nec9wF9X1XeSHAFsS/Jo99wtVfXFwcuTNG6LDoWq2gXs6pbfTvIsvR6SkpaxoVxTSHIq8FHgf7qh65M8mWRjkqPneM2GJFuTbP0F7w6jDElDMHAoJPl14H7gL6vqp8CtwBnAFL0jiZtne529JKWlaaBQSHIQvUD4WlX9G0BV7a6qvVX1HnA7cM7gZUoal0G+fQjwVeDZqvqnvvET+za7Anhq8eVJGrdBvn04D/gM8L0k27uxzwHrkkwBBbwMXDNQhZLGapBvH/4LyCxP2VRWWsa8o1FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUG+eFWAJK8DLwN7AX2VNXqJMcAXwdOpffjrZ+uqp8Mui9JozesI4WPV9VUVa3u1m8AtlTVKmBLty5pGRjV6cMaYFO3vAm4fET7kTRkwwiFAh5Jsi3Jhm7shK4B7Uwj2uOHsB9JYzDwNQXgvKrameR44NEkzy3kRV2AbAA4lMOGUIakYRj4SKGqdnaPrwMP0OsduXumfVz3+Posr7PBrLQEDdpg9vAkR8wsA5+k1zvyIWB9t9l64MFB9iNpfAY9fTgBeKDXa5YDgbuq6htJngDuTXI18Apw5YD7kTQmA4VCVb0E/O4s428AFw3y3pImwzsaJTUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSY9G/0ZjkI/T6Rc44Hfg74Cjgz4EfdeOfq6rNi65Q0lgtOhSq6nlgCiDJAcBr9Po+XAXcUlVfHEqFksZqWKcPFwEvVtUPh/R+kiZkWKGwFri7b/36JE8m2Zjk6CHtQ9IYDBwKSQ4GPgX8azd0K3AGvVOLXcDNc7xuQ5KtSbb+gncHLUPSkAzjSOFS4DtVtRugqnZX1d6qeg+4nV5vyf3YS1JamoYRCuvoO3WYaSzbuYJeb0lJy8RAbeOSHAZ8Arimb/gfkkwBBby8z3OSlrhBe0m+A/zGPmOfGagiSRPlHY2SGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqLCgUuk5Pryd5qm/smCSPJnmhezy6G0+SLyXZ0XWJOntUxUsavoUeKdwBXLLP2A3AlqpaBWzp1qHXHGZV97eBXscoScvEgkKhqh4D3txneA2wqVveBFzeN35n9TwOHLVPgxhJS9gg1xROqKpdAN3j8d34ScCrfdtNd2MNe0lKS9MoLjRmlrHab8BektKSNEgo7J45LegeX+/Gp4FT+rY7Gdg5wH4kjdEgofAQsL5bXg882Df+2e5biI8Bb82cZkha+hbUSzLJ3cCFwLFJpoHPA18A7k1yNfAKcGW3+WbgMmAH8A5w1ZBrljRCCwqFqlo3x1MXzbJtAdcNUpSkyfGORkkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1DAUJDUMBUkNQ0FSw1CQ1Jg3FOboI/mPSZ7rekU+kOSobvzUJD9Lsr37+8ooi5c0fAs5UriD/ftIPgr8dlX9DvB94Ma+516sqqnu79rhlClpXOYNhdn6SFbVI1W1p1t9nF7DF0krwDCuKfwZ8B9966cl+W6SbyU5f64X2UtSWpoW1PdhLkn+FtgDfK0b2gV8qKreSPJ7wL8nOauqfrrva6vqNuA2gCNzzH69JiVNxqKPFJKsB/4Q+JOuAQxV9W5VvdEtbwNeBD48jEIljceiQiHJJcDfAJ+qqnf6xo9LckC3fDqwCnhpGIVKGo95Tx/m6CN5I3AI8GgSgMe7bxouAP4+yR5gL3BtVb056xtLWpLmDYU5+kh+dY5t7wfuH7QoSZPjHY2SGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKlhKEhqGAqSGoaCpIahIKmx2F6SNyV5ra9n5GV9z92YZEeS55NcPKrCJY3GYntJAtzS1zNyM0CSM4G1wFnda74885PvkpaHRfWS/BXWAPd0TWF+AOwAzhmgPkljNsg1heu7VvQbkxzdjZ0EvNq3zXQ3JmmZWGwo3AqcAUzR6x95czeeWbadtU+kDWalpWlRoVBVu6tqb1W9B9zOL08RpoFT+jY9Gdg5x3vcVlWrq2r1QRyymDIkjcBie0me2Ld6BTDzzcRDwNokhyQ5jV4vyW8PVqKkcVpsL8kLk0zROzV4GbgGoKqeTnIv8Ay9FvXXVdXe0ZQuaRTSdZGfqCNzTJ2biyZdhrSifbPu21ZVq+fbzjsal6CHd26fdAn6ADMUliiDQZNiKCxhD+/cbjho7AyFZcBg0DgZCpIahsIy4dGCxsVQWEYMBo2DobDMGAwaNUNhGTIYNEqGwjJlMGhUDIVlzGDQKBgKy5w3OGnYDIUVwmDQsBgKkhqGwgri0YKGwVBYYQwGDcpQWIEMBg3CUFihDAYtlqGwghkMWozF9pL8el8fyZeTbO/GT03ys77nvjLK4jU/72PQ+zXvrznT6yX5z8CdMwNV9cczy0luBt7q2/7FqpoaVoEajod3bufi3/Rj0fzmDYWqeizJqbM9lyTAp4HfH25ZkiZl0GsK5wO7q+qFvrHTknw3ybeSnD/g+2uIPI3QQgwaCuuAu/vWdwEfqqqPAn8F3JXkyNleaC/JyTAYNJ9Fh0KSA4E/Ar4+M9a1oH+jW94GvAh8eLbX20tycgwG/SoLudA4lz8Anquq6ZmBJMcBb1bV3iSn0+sl+dKANX7geEFQk7SQryTvBv4b+EiS6SRXd0+tpT11ALgAeDLJ/wL3AddW1ZvDLFjSaC3k24d1c4z/6Sxj9wP3D16WpEnxjkZJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUsNQkNQwFCQ1DAVJDUNBUiNVNekaSPIj4P+AH0+6lhE7lpU9x5U+P1jec/ytqjpuvo2WRCgAJNlaVasnXccorfQ5rvT5wQdjjp4+SGoYCpIaSykUbpt0AWOw0ue40ucHH4A5LplrCpKWhqV0pCBpCZh4KCS5JMnzSXYkuWHS9QxL1437e1337a3d2DFJHk3yQvd49KTrfD/m6EA+65zS86Xuc30yydmTq3xh5pjfTUle6+ukflnfczd283s+ycWTqXr4JhoKSQ4A/gW4FDgTWJfkzEnWNGQfr6qpvq+wbgC2VNUqYEu3vpzcAVyyz9hcc7qUXjOgVcAG4NYx1TiIO9h/fgC3dJ/jVFVtBuj+n64Fzupe8+Xu//OyN+kjhXOAHVX1UlX9HLgHWDPhmkZpDbCpW94EXD7BWt63qnoM2Le5z1xzWgPcWT2PA0clOXE8lS7OHPObyxrgnq5V4g+AHfT+Py97kw6Fk4BX+9anu7GVoIBHkmxLsqEbO6GqdgF0j8dPrLrhmWtOK+mzvb47BdrYd8q3kubXmHQoZJaxlfJ1yHlVdTa9w+jrklww6YLGbKV8trcCZwBT9Lqq39yNr5T57WfSoTANnNK3fjKwc0K1DFVV7eweXwceoHdouXvmELp7fH1yFQ7NXHNaEZ9tVe2uqr1V9R5wO788RVgR85vNpEPhCWBVktOSHEzvws1DE65pYEkOT3LEzDLwSeApenNb3222HnhwMhUO1Vxzegj4bPctxMeAt2ZOM5aTfa6DXEHvc4Te/NYmOSTJafQuqH573PWNwiCt6AdWVXuSXA88DBwAbKyqpydZ05CcADyQBHr/xndV1TeSPAHc23XufgW4coI1vm9dB/ILgWOTTAOfB77A7HPaDFxG7wLcO8BVYy/4fZpjfhcmmaJ3avAycA1AVT2d5F7gGWAPcF1V7Z1E3cPmHY2SGpM+fZC0xBgKkhqGgqSGoSCpYShIahgKkhqGgqSGoSCp8f/M5BAb6ABlWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "# other_input_images = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "# other_target_masks = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "# print(np.array(input_images_rgb).shape)\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image, Right: Target mask\n",
    "# helper.plot_side_by_side([input_images, target_masks_rgb])\n",
    "x = target_masks[0, 2, :, :]\n",
    "something = plt.imshow(x)\n",
    "print(x)\n",
    "print(x.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROIList = [2, 3, 41, 42]\n",
    "def makeSeg(image):\n",
    "    \n",
    "    image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "    foreground = np.zeros(image.shape)\n",
    "    first = np.zeros(image.shape)\n",
    "    first[image == 2] = 1\n",
    "    foreground[image == 2] = 1\n",
    "    second = np.zeros(image.shape)\n",
    "    second[image == 3] = 1\n",
    "    foreground[image == 3] = 1\n",
    "    third = np.zeros(image.shape)\n",
    "    third[image == 41] = 1\n",
    "    foreground[image == 41] = 1\n",
    "    fourth = np.zeros(image.shape)\n",
    "    fourth[image == 42] = 1\n",
    "    foreground[image == 42] = 1\n",
    "    fifth = np.zeros(image.shape)\n",
    "    fifth[foreground == 0] = 1\n",
    "    \n",
    "    return np.concatenate((first, second, third, fourth, fifth), axis=2)\n",
    "\n",
    "dirs = './segmented_images/'\n",
    "files = os.listdir(dirs)\n",
    "# print(files[0])\n",
    "segmented_images = np.array([makeSeg(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "# print(segmented_images[0].max())\n",
    "# print(segmented_images[0].min())\n",
    "# print(segmented_images[0][80 : -80, 80 : -80])\n",
    "x = segmented_images[0][2]\n",
    "print(x.max())\n",
    "print(x.min())\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "# dirs = '/A/'\n",
    "# files = os.listdir(dirs)\n",
    "# data = [imread(files[i]) for i in range(len(files))]\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms, datasets, models\n",
    "\n",
    "# class SimDataset(Dataset):\n",
    "#     def __init__(self, count, transform=None):\n",
    "#         self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)        \n",
    "#         self.transform = transform\n",
    "#         print(self.input_images.shape)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.input_images)\n",
    "    \n",
    "#     def __getitem__(self, idx):        \n",
    "#         image = self.input_images[idx]\n",
    "#         mask = self.target_masks[idx]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         return [image, mask]\n",
    "\n",
    "# # use same transform for train/val for this example\n",
    "# trans = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "# ])\n",
    "\n",
    "# train_set = SimDataset(2000, transform=trans)\n",
    "# val_set = SimDataset(200, transform=trans)\n",
    "\n",
    "# image_datasets = {\n",
    "#     'train': train_set, 'val': val_set\n",
    "# }\n",
    "\n",
    "# batch_size = 25\n",
    "\n",
    "# dataloaders = {\n",
    "#     'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "#     'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# }\n",
    "\n",
    "# dataset_sizes = {\n",
    "#     x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "# }\n",
    "\n",
    "# dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 1500, 'val': 500}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function that reshapes the input image to have 1 as the number of channels\n",
    "def makeInput(image):\n",
    "    # print(image.shape)\n",
    "    return image.reshape((image.shape[0], image.shape[1], 1))\n",
    "\n",
    "# ROIList = [2, 3, 41, 42]\n",
    "def makeSeg(image):\n",
    "    \n",
    "    image = image.reshape((1, image.shape[0], image.shape[1]))\n",
    "    foreground = np.zeros(image.shape)\n",
    "    first = np.zeros(image.shape)\n",
    "    first[image == 2] = 1\n",
    "    foreground[image == 2] = 1\n",
    "    second = np.zeros(image.shape)\n",
    "    second[image == 3] = 1\n",
    "    foreground[image == 3] = 1\n",
    "    third = np.zeros(image.shape)\n",
    "    third[image == 41] = 1\n",
    "    foreground[image == 41] = 1\n",
    "    fourth = np.zeros(image.shape)\n",
    "    fourth[image == 42] = 1\n",
    "    foreground[image == 42] = 1\n",
    "    fifth = np.zeros(image.shape)\n",
    "    fifth[foreground == 0] = 1\n",
    "    \n",
    "    return np.concatenate((first, second, third, fourth, fifth), axis=0)\n",
    "    \n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        if train:\n",
    "            dirs = './original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            # print(files[0])\n",
    "            self.input_images = np.array([makeInput(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            # print(self.input_images.shape)\n",
    "            dirs = './segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        else:\n",
    "            dirs = './validation_original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.input_images = np.array([makeInput(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            dirs = './validation_segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(np.load(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            # transformation = self.transform\n",
    "            image = self.transform(image)\n",
    "            # mask = transformation(mask)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Resize(192), # Added this line\n",
    "    # transforms.RandomCrop(180), # Added this line\n",
    "    # transforms.RandomRotation(10), # Added this line\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(train=True, transform=trans)\n",
    "val_set = SimDataset(train=False, transform=trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 1, 256, 256]) torch.Size([25, 5, 256, 256])\n",
      "-2.1179039301310043 2.2489082969432315 -1.7080340953549602 0.7051683245442841\n",
      "0.0 1.0 0.2 0.40000000000000024\n"
     ]
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "# plt.imshow(reverse_transform(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 256, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AvgPool2d(kernel_size=7, stride=1, padding=0),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "    \n",
    "list(base_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "        AvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 107.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "summary(base_model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layersi = convrelu(1, 3,1,0)\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        layersi = self.layersi(input)\n",
    "        x_original = self.conv_original_size0(layersi)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(layersi)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]               6\n",
      "              ReLU-2          [-1, 3, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]           1,792\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "            Conv2d-5         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "            Conv2d-7         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-8         [-1, 64, 128, 128]             128\n",
      "              ReLU-9         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-10           [-1, 64, 64, 64]               0\n",
      "           Conv2d-11           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 64, 64]             128\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "           Conv2d-14           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 64, 64]             128\n",
      "             ReLU-16           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-17           [-1, 64, 64, 64]               0\n",
      "           Conv2d-18           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 64, 64]             128\n",
      "             ReLU-20           [-1, 64, 64, 64]               0\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "             ReLU-23           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-24           [-1, 64, 64, 64]               0\n",
      "           Conv2d-25          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-26          [-1, 128, 32, 32]             256\n",
      "             ReLU-27          [-1, 128, 32, 32]               0\n",
      "           Conv2d-28          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
      "           Conv2d-30          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-31          [-1, 128, 32, 32]             256\n",
      "             ReLU-32          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-33          [-1, 128, 32, 32]               0\n",
      "           Conv2d-34          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-35          [-1, 128, 32, 32]             256\n",
      "             ReLU-36          [-1, 128, 32, 32]               0\n",
      "           Conv2d-37          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "             ReLU-39          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-40          [-1, 128, 32, 32]               0\n",
      "           Conv2d-41          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-42          [-1, 256, 16, 16]             512\n",
      "             ReLU-43          [-1, 256, 16, 16]               0\n",
      "           Conv2d-44          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
      "           Conv2d-46          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-47          [-1, 256, 16, 16]             512\n",
      "             ReLU-48          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-49          [-1, 256, 16, 16]               0\n",
      "           Conv2d-50          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-51          [-1, 256, 16, 16]             512\n",
      "             ReLU-52          [-1, 256, 16, 16]               0\n",
      "           Conv2d-53          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-54          [-1, 256, 16, 16]             512\n",
      "             ReLU-55          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-56          [-1, 256, 16, 16]               0\n",
      "           Conv2d-57            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-58            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-62            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-63            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-64            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-65            [-1, 512, 8, 8]               0\n",
      "           Conv2d-66            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-67            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-68            [-1, 512, 8, 8]               0\n",
      "           Conv2d-69            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-70            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-71            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-72            [-1, 512, 8, 8]               0\n",
      "           Conv2d-73            [-1, 512, 8, 8]         262,656\n",
      "             ReLU-74            [-1, 512, 8, 8]               0\n",
      "         Upsample-75          [-1, 512, 16, 16]               0\n",
      "           Conv2d-76          [-1, 256, 16, 16]          65,792\n",
      "             ReLU-77          [-1, 256, 16, 16]               0\n",
      "           Conv2d-78          [-1, 512, 16, 16]       3,539,456\n",
      "             ReLU-79          [-1, 512, 16, 16]               0\n",
      "         Upsample-80          [-1, 512, 32, 32]               0\n",
      "           Conv2d-81          [-1, 128, 32, 32]          16,512\n",
      "             ReLU-82          [-1, 128, 32, 32]               0\n",
      "           Conv2d-83          [-1, 256, 32, 32]       1,474,816\n",
      "             ReLU-84          [-1, 256, 32, 32]               0\n",
      "         Upsample-85          [-1, 256, 64, 64]               0\n",
      "           Conv2d-86           [-1, 64, 64, 64]           4,160\n",
      "             ReLU-87           [-1, 64, 64, 64]               0\n",
      "           Conv2d-88          [-1, 256, 64, 64]         737,536\n",
      "             ReLU-89          [-1, 256, 64, 64]               0\n",
      "         Upsample-90        [-1, 256, 128, 128]               0\n",
      "           Conv2d-91         [-1, 64, 128, 128]           4,160\n",
      "             ReLU-92         [-1, 64, 128, 128]               0\n",
      "           Conv2d-93        [-1, 128, 128, 128]         368,768\n",
      "             ReLU-94        [-1, 128, 128, 128]               0\n",
      "         Upsample-95        [-1, 128, 256, 256]               0\n",
      "           Conv2d-96         [-1, 64, 256, 256]         110,656\n",
      "             ReLU-97         [-1, 64, 256, 256]               0\n",
      "           Conv2d-98          [-1, 5, 256, 256]             325\n",
      "================================================================\n",
      "Total params: 17,800,075\n",
      "Trainable params: 17,800,075\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 466.00\n",
      "Params size (MB): 67.90\n",
      "Estimated Total Size (MB): 534.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(5)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from loss import dice_loss\n",
    "\n",
    "# Function that calculates the Jaccard overlap between two images for a given ROI\n",
    "def JaccardOverlap(first, second, region):\n",
    "    firstOther = np.zeros(first.shape)\n",
    "    secondOther = np.zeros(second.shape)\n",
    "    \n",
    "    firstOther[first == region] = 1\n",
    "    secondOther[second == region] = 1\n",
    "    \n",
    "    firstOther = firstOther.astype(bool)\n",
    "    secondOther = secondOther.astype(bool)\n",
    "    \n",
    "    \n",
    "    firstCount = np.sum(firstOther)\n",
    "    secondCount = np.sum(secondOther)\n",
    "    \n",
    "    \n",
    "    intersectionCount = np.sum(np.bitwise_and(firstOther.reshape(first.size), secondOther.reshape(second.size)))\n",
    "    \n",
    "    return intersectionCount / (firstCount + secondCount - intersectionCount)\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred.float(), target.float())\n",
    "        \n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float) # Changed this line\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.float().to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 0/14\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d84e8934d976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-c4e88edd498f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-0c81a5bfa1cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_up0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_original\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_original_size2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn.{} is deprecated. Use nn.functional.interpolate instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but linear mode needs 3D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_bilinear2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2448\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trilinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but trilinear mode needs 5D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 5\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])\n",
    "print('Jaccard Overlap: ', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "ids = []\n",
    "data = []\n",
    "\n",
    "# Assuming the outputs are stored in a list called outList\n",
    "for i in range(7, len(outList) + 7):\n",
    "    if (i < 15):\n",
    "        ids.append(str(i) + '-left-wm') \n",
    "        data.append(rle_encode(outList[i][0]))\n",
    "        ids.append(str(i) + '-left-cortex')\n",
    "        data.append(rle_encode(outList[i][1]))\n",
    "        ids.append(str(i) + '-right-wm')\n",
    "        data.append(rle_encode(outList[i][2]))\n",
    "        ids.append(str(i) + '-right-cortex')\n",
    "        data.append(rle_encode(outList[i][3]))\n",
    "    else:\n",
    "        ids.append(str(i + 1) + '-left-wm') \n",
    "        data.append(rle_encode(outList[i][0]))\n",
    "        ids.append(str(i + 1) + '-left-cortex')\n",
    "        data.append(rle_encode(outList[i][1]))\n",
    "        ids.append(str(i + 1) + '-right-wm')\n",
    "        data.append(rle_encode(outList[i][2]))\n",
    "        ids.append(str(i + 1) + '-right-cortex')\n",
    "        data.append(rle_encode(outList[i][3]))\n",
    "\n",
    "### ids: list containing the 36 IDs as described above  ###\n",
    "### data: list containing the 36 binary segmentation masks in RLE format ###\n",
    "df = pd.DataFrame({\"Id\": labels, \"Predicted\": data})\n",
    "df.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
