{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(3, 192, 192, 3)\n",
      "0 255\n",
      "(3, 6, 192, 192)\n",
      "0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "# other_input_images = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "# other_target_masks = np.array([imread('./images/output_0_1.png'), imread('./images/output_2_2.png'), imread('./images/output_9_1.png')])\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "# print(np.array(input_images_rgb).shape)\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image, Right: Target mask\n",
    "helper.plot_side_by_side([input_images, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 192, 192, 3)\n",
      "(200, 192, 192, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 2000, 'val': 200}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dirs = '/A/'\n",
    "# files = os.listdir(dirs)\n",
    "# data = [imread(files[i]) for i in range(len(files))]\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms, datasets, models\n",
    "\n",
    "# class SimDataset(Dataset):\n",
    "#     def __init__(self, count, transform=None):\n",
    "#         self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)        \n",
    "#         self.transform = transform\n",
    "#         print(self.input_images.shape)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.input_images)\n",
    "    \n",
    "#     def __getitem__(self, idx):        \n",
    "#         image = self.input_images[idx]\n",
    "#         mask = self.target_masks[idx]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         return [image, mask]\n",
    "\n",
    "# # use same transform for train/val for this example\n",
    "# trans = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "# ])\n",
    "\n",
    "# train_set = SimDataset(2000, transform=trans)\n",
    "# val_set = SimDataset(200, transform=trans)\n",
    "\n",
    "# image_datasets = {\n",
    "#     'train': train_set, 'val': val_set\n",
    "# }\n",
    "\n",
    "# batch_size = 25\n",
    "\n",
    "# dataloaders = {\n",
    "#     'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "#     'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# }\n",
    "\n",
    "# dataset_sizes = {\n",
    "#     x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "# }\n",
    "\n",
    "# dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 256, 256, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 1500, 'val': 500}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function that reshapes the input image to have 1 as the number of channels\n",
    "def makeInput(image):\n",
    "    # print(image.shape)\n",
    "    return image.reshape((image.shape[0], image.shape[1], 1))\n",
    "\n",
    "# ROIList = [2, 3, 41, 42]\n",
    "def makeSeg(image):\n",
    "    \n",
    "    image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "    foreground = np.zeros(image.shape)\n",
    "    first = np.zeros(image.shape)\n",
    "    first[image == 2] == 1\n",
    "    foreground[image == 2] == 1\n",
    "    second = np.zeros(image.shape)\n",
    "    second[image == 3] == 1\n",
    "    foreground[image == 3] == 1\n",
    "    third = np.zeros(image.shape)\n",
    "    third[image == 41] == 1\n",
    "    foreground[image == 41] == 1\n",
    "    fourth = np.zeros(image.shape)\n",
    "    fourth[image == 42] == 1\n",
    "    foreground[image == 42] == 1\n",
    "    fifth = np.zeros(image.shape)\n",
    "    fifth[foreground == 0] == 1\n",
    "    \n",
    "    return np.concatenate((first, second, third, fourth, fifth), axis=2)\n",
    "    \n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        if train:\n",
    "            dirs = './original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            # print(files[0])\n",
    "            self.input_images = np.array([makeInput(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            print(self.input_images.shape)\n",
    "            dirs = './segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        else:\n",
    "            dirs = './validation_original_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.input_images = np.array([makeInput(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "            dirs = './validation_segmented_images/'\n",
    "            files = os.listdir(dirs)\n",
    "            self.target_masks = np.array([makeSeg(imread(dirs + str(files[i]))) for i in range(len(files))])\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            transformation = self.transform\n",
    "            image = transformation(image)\n",
    "            mask = transformation(mask)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Resize(192), # Added this line\n",
    "    # transforms.RandomCrop(180), # Added this line\n",
    "    # transforms.RandomRotation(10), # Added this line\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = SimDataset(train=True, transform=trans)\n",
    "val_set = SimDataset(train=False, transform=trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 1, 256, 256]) torch.Size([25, 5, 256, 256])\n",
      "-2.117904 2.2489083 -1.5695388 0.8372081\n",
      "-2.1179039301310043 0.0 -1.1916125320579487 0.9783626455077156\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1926067773cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-1926067773cf>\u001b[0m in \u001b[0;36mreverse_transform\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "plt.imshow(reverse_transform(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1, 256, 256])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AvgPool2d(kernel_size=7, stride=1, padding=0),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "base_model = models.resnet18(pretrained=False)\n",
    "    \n",
    "list(base_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "        AvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 107.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "summary(base_model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.base_layers = list(base_model.children())                \n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)        \n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)       \n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)        \n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)  \n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)        \n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)  \n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)  \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        layersi = convrelu(1, 3,1,0)(input)\n",
    "        x_original = self.conv_original_size0(layersi)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        \n",
    "        layer0 = self.layer0(layersi)            \n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)        \n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    " \n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)        \n",
    "        \n",
    "        out = self.conv_last(x)        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajay/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
      "              ReLU-7         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-8           [-1, 64, 64, 64]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 64, 64]             128\n",
      "             ReLU-11           [-1, 64, 64, 64]               0\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-15           [-1, 64, 64, 64]               0\n",
      "           Conv2d-16           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 64, 64]             128\n",
      "             ReLU-18           [-1, 64, 64, 64]               0\n",
      "           Conv2d-19           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 64, 64]             128\n",
      "             ReLU-21           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-22           [-1, 64, 64, 64]               0\n",
      "           Conv2d-23          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-24          [-1, 128, 32, 32]             256\n",
      "             ReLU-25          [-1, 128, 32, 32]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "           Conv2d-28          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
      "             ReLU-30          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-31          [-1, 128, 32, 32]               0\n",
      "           Conv2d-32          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-33          [-1, 128, 32, 32]             256\n",
      "             ReLU-34          [-1, 128, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-38          [-1, 128, 32, 32]               0\n",
      "           Conv2d-39          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-40          [-1, 256, 16, 16]             512\n",
      "             ReLU-41          [-1, 256, 16, 16]               0\n",
      "           Conv2d-42          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-43          [-1, 256, 16, 16]             512\n",
      "           Conv2d-44          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
      "             ReLU-46          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-47          [-1, 256, 16, 16]               0\n",
      "           Conv2d-48          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-49          [-1, 256, 16, 16]             512\n",
      "             ReLU-50          [-1, 256, 16, 16]               0\n",
      "           Conv2d-51          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-52          [-1, 256, 16, 16]             512\n",
      "             ReLU-53          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-54          [-1, 256, 16, 16]               0\n",
      "           Conv2d-55            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-56            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-57            [-1, 512, 8, 8]               0\n",
      "           Conv2d-58            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-59            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-60            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-62            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-63            [-1, 512, 8, 8]               0\n",
      "           Conv2d-64            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-65            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-66            [-1, 512, 8, 8]               0\n",
      "           Conv2d-67            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-68            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-69            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-70            [-1, 512, 8, 8]               0\n",
      "           Conv2d-71            [-1, 512, 8, 8]         262,656\n",
      "             ReLU-72            [-1, 512, 8, 8]               0\n",
      "         Upsample-73          [-1, 512, 16, 16]               0\n",
      "           Conv2d-74          [-1, 256, 16, 16]          65,792\n",
      "             ReLU-75          [-1, 256, 16, 16]               0\n",
      "           Conv2d-76          [-1, 512, 16, 16]       3,539,456\n",
      "             ReLU-77          [-1, 512, 16, 16]               0\n",
      "         Upsample-78          [-1, 512, 32, 32]               0\n",
      "           Conv2d-79          [-1, 128, 32, 32]          16,512\n",
      "             ReLU-80          [-1, 128, 32, 32]               0\n",
      "           Conv2d-81          [-1, 256, 32, 32]       1,474,816\n",
      "             ReLU-82          [-1, 256, 32, 32]               0\n",
      "         Upsample-83          [-1, 256, 64, 64]               0\n",
      "           Conv2d-84           [-1, 64, 64, 64]           4,160\n",
      "             ReLU-85           [-1, 64, 64, 64]               0\n",
      "           Conv2d-86          [-1, 256, 64, 64]         737,536\n",
      "             ReLU-87          [-1, 256, 64, 64]               0\n",
      "         Upsample-88        [-1, 256, 128, 128]               0\n",
      "           Conv2d-89         [-1, 64, 128, 128]           4,160\n",
      "             ReLU-90         [-1, 64, 128, 128]               0\n",
      "           Conv2d-91        [-1, 128, 128, 128]         368,768\n",
      "             ReLU-92        [-1, 128, 128, 128]               0\n",
      "         Upsample-93        [-1, 128, 256, 256]               0\n",
      "           Conv2d-94         [-1, 64, 256, 256]         110,656\n",
      "             ReLU-95         [-1, 64, 256, 256]               0\n",
      "           Conv2d-96          [-1, 5, 256, 256]             325\n",
      "================================================================\n",
      "Total params: 17,800,069\n",
      "Trainable params: 17,800,069\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 463.00\n",
      "Params size (MB): 67.90\n",
      "Estimated Total Size (MB): 531.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check keras-like model summary using torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetUNet(5)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from loss import dice_loss\n",
    "\n",
    "# Function that calculates the Jaccard overlap between two images for a given ROI\n",
    "def JaccardOverlap(first, second, region):\n",
    "    firstOther = np.zeros(first.shape)\n",
    "    secondOther = np.zeros(second.shape)\n",
    "    \n",
    "    firstOther[first == region] = 1\n",
    "    secondOther[second == region] = 1\n",
    "    \n",
    "    firstOther = firstOther.astype(bool)\n",
    "    secondOther = secondOther.astype(bool)\n",
    "    \n",
    "    \n",
    "    firstCount = np.sum(firstOther)\n",
    "    secondCount = np.sum(secondOther)\n",
    "    \n",
    "    \n",
    "    intersectionCount = np.sum(np.bitwise_and(firstOther.reshape(first.size), secondOther.reshape(second.size)))\n",
    "    \n",
    "    return intersectionCount / (firstCount + secondCount - intersectionCount)\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred.double(), target.double())\n",
    "        \n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float) # Changed this line\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 0/14\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544176307774/work/aten/src/TH/THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d84e8934d976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-dbb9066144de>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ECE4250_Assignments/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-8e29af778a80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_original\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_original_size2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544176307774/work/aten/src/TH/THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 5\n",
    "\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)        \n",
    "        \n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])\n",
    "print('Jaccard Overlap: ', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
